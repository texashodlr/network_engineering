<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:custom="https://www.oreilly.com/rss/custom" version="2.0">
<channel>
<title>Radar</title>
<atom:link href="https://www.oreilly.com/radar/feed/" rel="self" type="application/rss+xml"/>
<link>https://www.oreilly.com/radar</link>
<description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
<lastBuildDate>Thu, 29 May 2025 15:38:10 +0000</lastBuildDate>
<language>en-US</language>
<sy:updatePeriod> hourly </sy:updatePeriod>
<sy:updateFrequency> 1 </sy:updateFrequency>
<generator>https://wordpress.org/?v=6.8.1</generator>
<image>
<url>https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/04/cropped-favicon_512x512-32x32.png</url>
<title>Radar</title>
<link>https://www.oreilly.com/radar</link>
<width>32</width>
<height>32</height>
</image>
<item>
<title>Generative AI in the Real World: Danielle Belgrave on Generative AI in Pharma and Medicine</title>
<link>https://www.oreilly.com/radar/generative-ai-in-the-real-world-danielle-belgrave-on-generative-ai-in-pharma-and-medicine/</link>
<comments>https://www.oreilly.com/radar/generative-ai-in-the-real-world-danielle-belgrave-on-generative-ai-in-pharma-and-medicine/#respond</comments>
<pubDate>Thu, 29 May 2025 10:28:45 +0000</pubDate>
<dc:creator>
<![CDATA[ Ben Lorica and Danielle Belgrave ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Commentary ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16800</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2024/01/Podcast_Cover_GenAI_in_the_Real_World.png" medium="image" type="image/png"/>
<description>
<![CDATA[ Join Danielle Belgrave and Ben Lorica for a discussion of AI in healthcare. Danielle is VP of AI and machine learning at GSK (formerly GlaxoSmithKline). She and Ben discuss using AI and machine learning to get better diagnoses that reflect the differences between patients. Listen in to learn about the challenges of working with health [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>Join Danielle Belgrave and Ben Lorica for a discussion of AI in healthcare. Danielle is VP of AI and machine learning at GSK (formerly GlaxoSmithKline). She and Ben discuss using AI and machine learning to get better diagnoses that reflect the differences between patients. Listen in to learn about the challenges of working with health data—a field where there’s both too much data and too little, and where hallucinations have serious consequences. And if you’re excited about healthcare, you’ll also find out how AI developers can get into the field.</p> <p>Check out <a href="https://learning.oreilly.com/playlists/42123a72-1108-40f1-91c0-adbfb9f4983b/?_gl=1*16z5k2y*_ga*MTE1NDE4NjYxMi4xNzI5NTkwODkx*_ga_092EL089CH*MTcyOTYxNDAyNC4zLjEuMTcyOTYxNDAyNi41OC4wLjA." target="_blank" rel="noreferrer noopener">other episodes</a> of this podcast on the O’Reilly learning platform.</p> <p><strong>About the <em>Generative AI in the Real World</em> podcast:</strong> In 2023, ChatGPT put AI on everyone’s agenda. In 2025, the challenge will be turning those agendas into reality. In <em>Generative AI in the Real World</em>, Ben Lorica interviews leaders who are building with AI. Learn from their experience to help put AI to work in your enterprise.</p> <h3 class="wp-block-heading">Points of Interest</h3> <ul class="wp-block-list"> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=0" target="_blank" rel="noreferrer noopener">0:00</a>: Introduction to Danielle Belgrave, VP of AI and machine learning at GSK. Danielle is our first guest representing Big Pharma. It will be interesting to see how people in pharma are using AI technologies.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=49" target="_blank" rel="noreferrer noopener">0:49</a>: My interest in machine learning for healthcare began 15 years ago. My PhD was on understanding patient heterogeneity in asthma-related disease. This was before electronic healthcare records. By leveraging different kinds of data, genomics data and biomarkers from children, and seeing how they developed asthma and allergic diseases, I developed causal modeling frameworks and graphical models to see if we could identify who would respond to what treatments. This was quite novel at the time. We identified five different types of asthma. If we can understand heterogeneity in asthma, a bigger challenge is understanding heterogeneity in mental health. The idea was trying to understand heterogeneity over time in patients with anxiety.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=252" target="_blank" rel="noreferrer noopener">4:12</a>: When I went to DeepMind, I worked on the healthcare portfolio. I became very curious about how to understand things like MIMIC, which had electronic healthcare records, and image data. The idea was to leverage tools like active learning to minimize the amount of data you take from patients. We also published work on improving the diversity of datasets.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=319" target="_blank" rel="noreferrer noopener">5:19</a>: When I came to GSK, it was an exciting opportunity to do both tech and health. Health is one of the most challenging landscapes we can work on. Human biology is very complicated. There is so much random variation. To understand biology, genomics, disease progression, and have an impact on how drugs are given to patients is amazing.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=375" target="_blank" rel="noreferrer noopener">6:15</a>: My role is leading AI/ML for clinical development. How can we understand heterogeneity in patients to optimize clinical trial recruitment and make sure the right patients have the right treatment?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=416" target="_blank" rel="noreferrer noopener">6:56</a>: Where does AI create the most value across GSK today? That can be both traditional AI and generative AI.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=443" target="_blank" rel="noreferrer noopener">7:23</a>: I use everything interchangeably, though there are distinctions. The real important thing is focusing on the problem we are trying to solve, and focusing on the data. How do we generate data that’s meaningful? How do we think about deployment?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=487" target="_blank" rel="noreferrer noopener">8:07</a>: And all the Q&amp;A and red teaming.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=500" target="_blank" rel="noreferrer noopener">8:20</a>: It’s hard to put my finger on what’s the most impactful use case. When I think of the problems I care about, I think about oncology, pulmonary disease, hepatitis—these are all very impactful problems, and they’re problems that we actively work on. If I were to highlight one thing, it’s the interplay between when we are looking at whole genome sequencing data and looking at molecular data and trying to translate that into computational pathology. By looking at those data types and understanding heterogeneity at that level, we get a deeper biological representation of different subgroups and understand mechanisms of action for response to drugs.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=575" target="_blank" rel="noreferrer noopener">9:35</a>: It’s not scalable doing that for individuals, so I’m interested in how we translate across different types or modalities of data. Taking a biopsy—that’s where we’re entering the field of artificial intelligence. How do we translate between genomics and looking at a tissue sample?&nbsp;&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=625" target="_blank" rel="noreferrer noopener">10:25</a>: If we think of the impact of the clinical pipeline, the second example would be using generative AI to discover drugs, target identification. Those are often in silico experiments. We have perturbation models. Can we perturb the cells? Can we create embeddings that will give us representations of patient response?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=673" target="_blank" rel="noreferrer noopener">11:13</a>: We’re generating data at scale. We want to identify targets more quickly for experimentation by ranking probability of success.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=696" target="_blank" rel="noreferrer noopener">11:36</a>: You’ve mentioned multimodality a lot. This includes computer vision, images. What other modalities?&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=713" target="_blank" rel="noreferrer noopener">11:53</a>: Text data, health records, responses over time, blood biomarkers, RNA-Seq data. The amount of data that has been generated is quite incredible. These are all different data modalities with different structures, different ways of correcting for noise, batch effects, and understanding human systems.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=771" target="_blank" rel="noreferrer noopener">12:51</a>: When you run into your former colleagues at DeepMind, what kinds of requests do you give them?&nbsp;&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=794" target="_blank" rel="noreferrer noopener">13:14</a>: Forget about the chatbots. A lot of the work that’s happening around large language models—thinking of LLMs as productivity tools that can help. But there has also been a lot of exploration around building larger frameworks where we can do inference. The challenge is around data. Health data is very sparse. That’s one of the challenges. How do we fine-tune models to specific solutions or specific disease areas or specific modalities of data? There’s been a lot of work on foundation models for computational pathology or foundations for single cell structure. If I had one wish, it would be looking at small data and how do you have robust patient representations when you have small datasets? We’re generating large amounts of data on small numbers of patients. This is a big methodological challenge. That’s the North Star.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=912" target="_blank" rel="noreferrer noopener">15:12</a>: When you describe using these foundation models to generate synthetic data, what guardrails do you put in place to prevent hallucination?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=930" target="_blank" rel="noreferrer noopener">15:30</a>: We’ve had a responsible AI team since 2019. It’s important to think of those guardrails especially in health, where the rewards are high but so are the stakes. One of the things the team has implemented is AI principles, but we also use model cards. We have policymakers understanding the consequences of the work; we also have engineering teams. There’s a team that looks precisely at understanding hallucinations with the language model we’ve built internally, called Jules.<sup>1</sup> There’s been a lot of work looking at metrics of hallucination and accuracy for those models. We also collaborate on things like interpretability and building reusable pipelines for responsible AI. How can we identify the blind spots in our analysis?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1062" target="_blank" rel="noreferrer noopener">17:42</a>: Last year, a lot of people started doing fine-tuning, RAG, and GraphRAG; I assume you do all of these?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1085" target="_blank" rel="noreferrer noopener">18:05</a>: RAG happens a lot in the responsible AI team. We have built a knowledge graph. That was one of the earliest knowledge graphs—before I joined. It’s maintained by another team at the moment. We have a platforms team that deals with all the scaling and deploying across the company. Tools like knowledge graph aren’t just AI/ML. Also Jules—it’s maintained outside AI/ML. It’s exciting when you see these solutions scale.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1202" target="_blank" rel="noreferrer noopener">20:02</a>: The buzzy term this year is agents and even multi-agents. What is the state of agentic AI within GSK?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1218" target="_blank" rel="noreferrer noopener">20:18</a>: We’ve been working on this for quite a while, especially within the context of large language models. It allows us to leverage a lot of the data that we have internally, like clinical data. Agents are built around those datatypes and the different modalities of questions that we have. We’ve built agents for genetic data or lab experimental data. An orchestral agent in Jules can combine those different agents in order to draw inferences. That landscape of agents is really important and relevant. It gives us refined models on individual questions and types of modalities.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1288" target="_blank" rel="noreferrer noopener">21:28</a>: You alluded to personalized medicine. We’ve been talking about that for a long time. Can you give us an update? How will AI accelerate that?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1314" target="_blank" rel="noreferrer noopener">21:54</a>: This is a field I’m really optimistic about. We have had a lot of impact; sometimes when you have your nose to the glass, you don’t see it. But we’ve come a long way. First, through data: We have exponentially more data than we had 15 years ago. Second, compute power: When I started my PhD, the fact that I had a GPU was amazing. The scale of computation has accelerated. And there has been a lot of influence from science as well. There has been a Nobel Prize for protein folding. Understanding of human biology is something we’ve pushed the needle on. A lot of the Nobel Prizes were about understanding biological mechanisms, understanding basic science. We’re currently on building blocks towards that. It took years to get from understanding the ribosome to understanding the mechanism for HIV.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1435" target="_blank" rel="noreferrer noopener">23:55</a>: In AI for healthcare, we’ve seen more immediate impacts. Just the fact of understanding something heterogeneous: If we both get a diagnosis of asthma, that will have different manifestations, different triggers. That understanding of heterogeneity in things like mental health: We are different; things need to be treated differently. We also have the ecosystem, where we can have an impact. We can impact clinical trials. We are in the pipeline for drugs. </li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1539" target="_blank" rel="noreferrer noopener">25:39</a>: One of the pieces of work we’ve published has been around understanding differences in response to the drug for hepatitis B.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1561" target="_blank" rel="noreferrer noopener">26:01</a>: You’re in the UK, you have the NHS. In the US, we still have the data silo problem: You go to your primary care, and then a specialist, and they have to communicate using records and fax. How can I be optimistic when systems don’t even talk to each other?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1596" target="_blank" rel="noreferrer noopener">26:36</a>: That’s an area where AI can help. It’s not a problem I work on, but how can we optimize workflow? It’s a systems problem.</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1619" target="_blank" rel="noreferrer noopener">26:59</a>: We all associate data privacy with healthcare. When people talk about data privacy, they get sci-fi, with homomorphic encryption and federated learning. What’s reality? What’s in your daily toolbox?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1654" target="_blank" rel="noreferrer noopener">27:34</a>: These tools are not necessarily in my daily toolbox. Pharma is heavily regulated; there’s a lot of transparency around the data we collect, the models we built. There are platforms and systems and ways of ingesting data. If you have a collaboration, you often work with a trusted research environment. Data doesn’t necessarily leave. We do analysis of data in their trusted research environment, we make sure everything is privacy preserving and we’re respecting the guardrails.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1751" target="_blank" rel="noreferrer noopener">29:11</a>: Our listeners are mainly software developers. They may wonder how they enter this field without any background in science. Can they just use LLMs to speed up learning? If you were trying to sell an ML developer on joining your team, what kind of background do they need?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1791" target="_blank" rel="noreferrer noopener">29:51</a>: You need a passion for the problems that you’re solving. That’s one of the things I like about GSK. We don’t know everything about biology, but we have very good collaborators.&nbsp;</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1820" target="_blank" rel="noreferrer noopener">30:20</a>: Do our listeners need to take biochemistry? Organic chemistry?</li> <li><a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3#t=1824" target="_blank" rel="noreferrer noopener">30:24</a>: No, you just need to talk to scientists. Get to know the scientists, hear their problems. We don’t work in silos as AI researchers. We work with the scientists. A lot of our collaborators are doctors, and have joined GSK because they want to have a bigger impact.</li> </ul> <hr class="wp-block-separator has-alpha-channel-opacity"/> <h3 class="wp-block-heading">Footnotes</h3> <ol class="wp-block-list"> <li>Not to be confused with Google’s recent agentic coding announcement.</li> </ol> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/generative-ai-in-the-real-world-danielle-belgrave-on-generative-ai-in-pharma-and-medicine/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
<enclosure url="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/GenAI_in_the_Real_World_with_Danielle_Belgrave.mp3" length="45585857" type="audio/mpeg"/>
</item>
<item>
<title>AI First Puts Humans First</title>
<link>https://www.oreilly.com/radar/ai-first-puts-humans-first/</link>
<comments>https://www.oreilly.com/radar/ai-first-puts-humans-first/#respond</comments>
<pubDate>Wed, 28 May 2025 10:04:52 +0000</pubDate>
<dc:creator>
<![CDATA[ Tim O’Reilly ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Commentary ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16712</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/na-polygons-1a-1400x950-1.jpg" medium="image" type="image/jpeg"/>
<description>
<![CDATA[ While I prefer “AI native” to describe the product development approach centered on AI that we’re trying to encourage at O’Reilly, I’ve sometimes used the term “AI first” in my communications with O’Reilly staff. And so I was alarmed and dismayed to learn that in the press, that term has now come to mean “using [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>While I prefer “AI native” to describe the product development approach centered on AI that we’re trying to encourage at O’Reilly, I’ve sometimes used the term “AI first” in my communications with O’Reilly staff. And so I was alarmed and dismayed to learn that in the press, that term has now come to mean “<a href="https://www.fastcompany.com/91332763/going-ai-first-appears-to-be-backfiring-on-klarna-and-duolingo" target="_blank" rel="noreferrer noopener">using AI to replace people</a>.” Many Silicon Valley investors and entrepreneurs even seem to view <a href="https://www.theguardian.com/commentisfree/2025/may/12/for-silicon-valley-ai-isnt-just-about-replacing-some-jobs-its-about-replacing-all-of-them" target="_blank" rel="noreferrer noopener">putting people out of work as a massive opportunity</a>.</p> <p>That idea is anathema to me. It’s also wrong, both morally and practically. The whole thrust of my 2017 book <a href="https://www.oreilly.com/tim/wtf-book.html" target="_blank" rel="noreferrer noopener"><em>WTF? What’s the Future and Why It’s Up to Us</em></a> was that rather than using technology to replace workers, we can augment them so that they can do things that were previously impossible. It’s not as though there aren’t still untold problems to solve, new products and experiences to create, and ways to make the world better, not worse.</p> <p>Every company is facing this choice today. Those that use AI simply to reduce costs and replace workers will be outcompeted by those that use it to expand their capabilities. So, for example, at O’Reilly, we have primarily offered our content in English, with only the most popular titles translated into the most commercially viable languages. But now, with the aid of AI, we can translate <em>everything</em> into—well, not <em>every</em> language (yet)—dozens of languages, making our knowledge and our products accessible and affordable in parts of the world that we just couldn’t serve before. These AI-only translations are not as good as those that are edited and curated by humans, but an AI-generated translation is better than no translation. Our customers who don’t speak English are delighted to have access to technical learning in their own language.</p> <p>As another example, we have built quizzes, summaries, audio, and other AI-generated content—not to mention AI-enabled search and answers—using new workflows that involve our editors, instructional designers, authors, and trainers in shaping the generation and the evaluation of these AI generated products. Not only that, we <a href="https://www.oreilly.com/radar/the-new-oreilly-answers-the-r-in-rag-stands-for-royalties/" target="_blank" rel="noreferrer noopener">pay royalties to authors</a> on these derivative products.</p> <p>But these things are really not yet what I call “AI native.” What do I mean by that?</p> <p>I’ve been around a lot of user interface transitions: from the CRT screen to the GUI, from the GUI to the web, from the web on desktops and laptops to mobile devices. We all remember the strategic conversations about “mobile first.” Many companies were late to the party in realizing that consumer expectations had shifted, and that if you didn’t have an app or web interface that worked well on mobile phones, you’d quickly lose your customers. They lost out to companies that quickly embraced the new paradigm.</p> <p>“Mobile first” meant prioritizing user experiences for a small device, and scaling up to larger screens. At first, companies simply tried to downsize their existing systems (remember Windows Mobile?) or somehow shoehorn their desktop interface onto a small touchscreen. That didn’t work. The winners were companies like Apple that created systems and interfaces that treated the mobile device as a primary means of user interaction.</p> <p>We have to do the same with AI. When we simply try to implement what we’ve done before, using AI to do it more quickly and cost-efficiently, we might see some cost savings, but we will utterly fail to surprise and delight our customers. Instead, we have to re-envision what we do, to ask ourselves how we might do it with AI if we were coming fresh to the problem with this new toolkit.</p> <p>Chatbots&nbsp;like ChatGPT and Claude have completely reset user expectations. The long arc of user interfaces to computers is to bring them closer and closer to the way humans communicate with each other. We went from having to “speak computer” (literally binary code in some of the earliest stored program computers) to having them understand human language.</p> <p>In some ways, we had started doing this with keyword search. We’d put in human words and get back documents that the algorithm thought were most related to what we were looking for. But it was still a limited pidgin.</p> <p>Now, though, we can talk to a search engine (or chatbot) in a much fuller way, not just in natural language, but, with the right preservation of context, in a multi-step conversation, or with a range of questions that goes well beyond traditional search. For example, in searching the O’Reilly platform’s books, videos, and live online courses, we might ask something like: “What are the differences between Camille Fournier’s book <em>The Manager’s Path</em> and Addy Osmani’s <em>Leading Effective Engineering Teams</em>?” Or “What are the most popular books, courses, and live trainings on the O&#8217;Reilly platform about software engineering soft skills?” followed by the clarification, “What I really want is something that will help me prepare for my next job interview.”</p> <p>Or consider “verifiable skills”—one of the major features that corporate learning offices demand of platforms like ours. In the old days, certifications and assessments mostly relied on multiple-choice questions, which we all know are a weak way to assess skills, and which users aren’t that fond of.</p> <p>Now, with AI, we might ask AI to assess a programmer’s skills and suggest opportunities for improvement based on their code repository or other proof of work. Or an AI can watch a user’s progress through a coding assignment in a course and notice not just what the user “got wrong<s>,</s>” but what parts they flew through and which ones took longer because they needed to do research or ask questions of their AI mentor. An AI native assessment methodology not only does more, it does it seamlessly, as part of a far superior user experience.</p> <p>We haven’t rolled out all these new features. But these are the kind of AI native things we are trying to do, things that were completely impossible before we had a still largely unexplored toolbox that daily is filled with new power tools. As you can see, what we’re really trying to do is to use AI to make the interactions of our customers with our content richer and more natural. In short, more human.</p> <p>One mistake that we’ve been trying to avoid is what might be called “putting new wine in old bottles.” That is, there’s a real temptation for those of us with years of experience designing for the web and mobile to start with a mockup of a web application interface, with a window where the AI interaction takes place. This is where I think “AI first” really is the right term. I like to see us prototyping the interaction with AI <em>before</em> thinking about what kind of web or mobile interface to wrap around it. When you test out actual AI-first interactions, they may give you completely different ideas about what the right interface to wrap around it might look like.</p> <p>There’s another mistake to avoid, which is to expect an AI to be able to do magic and not think deeply enough about all the hard work of evaluation, creation of guardrails, interface design, cloud deployment, security, and more. “AI native” does not mean “AI only.” Every AI application is a hybrid application. I’ve been very taken with Phillip Carter’s post, <a href="https://www.phillipcarter.dev/posts/llms-computers" target="_blank" rel="noreferrer noopener">LLMs Are Weird Computers</a>, which makes the point that we’re now programming with two fundamentally different types of computers: one that can write poetry but struggles with basic arithmetic, another that calculates flawlessly but can’t interact easily with humans in our own native languages. The art of modern development is orchestrating these systems to complement each other.</p> <p>This was a major theme of our recent AI Codecon <a href="https://www.oreilly.com/radar/takeaways-from-coding-with-ai/" target="_blank" rel="noreferrer noopener">Coding with AI</a>. The lineup of expert practitioners explained how they are bringing AI into their workflow in innovative ways to accelerate (not replace) their productivity and their creativity. And speaker after speaker reminded us of what each of us still needs to bring to the table.</p> <p>Chelsea Troy <a href="https://youtu.be/bg4z70cOOF4" target="_blank" rel="noreferrer noopener">put it beautifully</a>:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p>Large language models have not wholesale wiped out programming jobs so much as they have called us to a more advanced, more contextually aware, and more communally oriented skill set that we frankly were already being called to anyway…. On relatively simple problems, we can get away with outsourcing some of our judgment. As the problems become more complicated, we can&#8217;t.</p> </blockquote> <p>The problems of integrating AI into our businesses, our lives, and our society are indeed complicated. But whether you call it “AI native” or “AI first,” it does not mean embracing the cult of “economic efficiency” that reduces humans to a cost to be eliminated.</p> <p>No, it means doing more, using humans augmented with AI to solve problems that were previously impossible, in ways that were previously unthinkable, and in ways that make our machine systems more attuned to the humans they are meant to serve. As Chelsea said, we are called to integrate AI into&nbsp; “a more advanced, more contextually aware, and more communally oriented” sensibility. AI first puts humans first.</p> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/ai-first-puts-humans-first/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
</item>
<item>
<title>MCP: What It Is and Why It Matters—Part 2</title>
<link>https://www.oreilly.com/radar/mcp-what-it-is-and-why-it-matters-part-2/</link>
<comments>https://www.oreilly.com/radar/mcp-what-it-is-and-why-it-matters-part-2/#respond</comments>
<pubDate>Tue, 27 May 2025 10:13:14 +0000</pubDate>
<dc:creator>
<![CDATA[ Addy Osmani ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Research ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16692</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/na-synapse-3a-1400x950-1.jpg" medium="image" type="image/jpeg"/>
<description>
<![CDATA[ This is the second of four parts in this series. Part 1 can be found here. 4. The Architecture of MCP: Clients, Protocol, Servers, and Services How does MCP actually work under the hood? At its core, MCP follows a client–server architecture, with a twist tailored for AI-to-software communication. Let’s break down the roles: MCP [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p class="has-black-color has-text-color has-background has-link-color wp-elements-92fb58f0b6aab6165d3c69cb5f292627" style="background:linear-gradient(135deg,rgb(169,184,195) 74%,rgb(238,238,238) 100%)"><em>This is the second of four parts in this series. Part 1 can be found </em><a href="https://www.oreilly.com/radar/mcp-what-it-is-and-why-it-matters-part-1/" target="_blank" rel="noreferrer noopener"><em>here</em></a><em>.</em></p> <h2 class="wp-block-heading"><strong>4. The Architecture of MCP: Clients, Protocol, Servers, and Services</strong></h2> <p>How does MCP actually work under the hood? At its core, MCP follows a <strong>client–server architecture</strong>, with a twist tailored for AI-to-software communication. Let’s break down the roles:</p> <h4 class="wp-block-heading"><strong>MCP servers</strong></h4> <p>These are lightweight adapters that run alongside a specific application or service. An MCP server exposes that application’s functionality (its “services”) in a standardized way. Think of the server as a <strong>translator embedded in the app</strong>—it knows how to take a natural-language request (from an AI) and perform the equivalent action in the app. For example, a Blender MCP server knows how to map “create a cube and apply a wood texture” onto Blender’s Python API calls. Similarly, a GitHub MCP server can take “list my open pull requests” and fetch that via the GitHub API. MCP servers typically implement a few key things:</p> <ul class="wp-block-list"> <li><strong>Tool discovery:</strong> They can describe what actions/capabilities the application offers (so the AI knows what it can ask for).</li> <li><strong>Command parsing:</strong> They interpret incoming instructions from the AI into precise application commands or API calls.</li> <li><strong>Response formatting:</strong> They take the output from the app (data, confirmation messages, etc.) and format it back in a way the AI model can understand (usually as text or structured data).</li> <li><strong>Error handling:</strong> They catch exceptions or invalid requests and return useful error messages for the AI to adjust.</li> </ul> <h4 class="wp-block-heading"><strong>MCP clients</strong></h4> <p>On the other side, an AI assistant (or the platform hosting it) includes an MCP client component. This client maintains a <strong>1:1 connection to an MCP server</strong>. In simpler terms, if the AI wants to use a particular tool, it will connect through an MCP client to that tool’s MCP server. The client’s job is to handle the communication (open a socket, send/receive messages) and present the server’s responses to the AI model. Many AI “host” programs act as an MCP client manager—e.g., Cursor (an AI IDE) can spin up an MCP client to talk to Figma’s server or Ableton’s server, as configured. The <strong>MCP client and server speak the same protocol</strong>, exchanging messages back and forth.</p> <h4 class="wp-block-heading"><strong>The MCP protocol</strong></h4> <p>This is the <strong>language and rules</strong> that the clients and servers use to communicate. It defines things like message formats, how a server advertises its available commands, how an AI asks a question or issues a command, and how results are returned. The protocol is transport agnostic: It can work over <strong>HTTP/WebSocket for remote or stand-alone servers, or even standard I/O streams (stdin/stdout) for local integrations</strong>. The content of the messages might be JSON or another structured schema. (The spec uses JSON Schema for definitions.) Essentially, the protocol ensures that whether an AI is talking to a design tool or a database, the <strong>handshake and query formats</strong> are consistent. This consistency is why an AI can switch from one MCP server to another without custom coding—the <strong>“grammar” of interaction remains the same</strong>.</p> <h4 class="wp-block-heading"><strong>Services (applications/data sources)</strong></h4> <p>These are the actual apps, databases, or systems that the MCP servers interface with. We call them “services” or data sources—they are the <strong>end target</strong> the AI ultimately wants to utilize. They can be <strong>local</strong> (e.g., your filesystem, an Excel file on your computer, a running Blender instance) or <strong>remote</strong> (e.g., a SaaS app like Slack or GitHub accessed via API). The MCP server is responsible for securely accessing these services on behalf of the AI. For example, a local service might be a directory of documents (served via a Filesystem MCP), whereas a remote service could be a third-party API (like Zapier’s web API for thousands of apps, which we’ll discuss later). In MCP’s architecture diagrams, you’ll often see both <strong>local data sources and remote services</strong>—MCP is designed to handle both, meaning an AI can pull from your <strong>local context</strong> (files, apps) and <strong>online context</strong> seamlessly.</p> <p>To illustrate the flow, imagine you tell your AI assistant (in Cursor), “Hey, gather the user stats from our product’s database and generate a bar chart.” Cursor (as an MCP host) might have an <strong>MCP client</strong> for the database (say a Postgres MCP server) and another for a visualization tool. The query goes to the Postgres <strong>MCP server</strong>, which runs the actual SQL and returns the data. Then the AI might send that data to the visualization tool’s <strong>MCP server</strong> to create a chart image. Each of these steps is mediated by the MCP protocol, which handles discovering what the AI can do (“this server offers a run_query action”), invoking it, and returning results. All the while, the AI model doesn’t have to know SQL or the plotting library’s API—it just uses natural language and the <strong>MCP servers translate its intent into action</strong>.</p> <p>It’s worth noting that <strong>security and control</strong> are part of architecture considerations. MCP servers run with certain permissions—for instance, a GitHub MCP server might have a token that grants read access to certain repos. Currently, configuration is manual, but the architecture anticipates adding standardized authentication in the future for robustness (more on that later). Also, <strong>communication channels</strong> are flexible: Some integrations run the MCP server inside the application process (e.g., a Unity plug-in that opens a local port), while others run as separate processes. In all cases, the architecture cleanly separates the concerns: The application side (server) and the AI side (client) meet through the protocol “in the middle.”</p> <h2 class="wp-block-heading"><strong>5. Why MCP Is a Game Changer for AI Agents and Developer Tooling</strong></h2> <p>MCP is a fundamental shift that could <strong>reshape how we build software and use AI</strong>. For AI agents, MCP is transformative because it <strong>dramatically expands their reach</strong> while simplifying their design. Instead of hardcoding capabilities, an AI agent can now <strong>dynamically discover and use new tools</strong> via MCP. This means we can easily give an AI assistant new powers by spinning up an MCP server, without retraining the model or altering the core system. It’s analogous to how adding a new app to your smartphone suddenly gives you new functionality—here, adding a new MCP server instantly teaches your AI a new skill set.</p> <p>From a developer tooling perspective, the implications are huge. <strong>Developer workflows often span dozens of tools</strong>: coding in an IDE, using GitHub for code, Jira for tickets, Figma for design, CI pipelines, browsers for testing, etc. With MCP, an AI codeveloper can hop between all these seamlessly, acting as the glue. This unlocks “composable” workflows where complex tasks are automated by the AI chaining actions across tools. For example, consider integrating design with code: With an MCP connection, your AI IDE can<a href="https://github.com/sonnylazuardi/cursor-talk-to-figma-mcp" target="_blank" rel="noreferrer noopener"> pull design specs from Figma and generate code</a>, eliminating manual steps and potential miscommunications.</p> <p>No more context switching, no more manual translations, no more design-to-code friction—the AI can directly read design files, create UI components, and even export assets, all without leaving the coding environment.</p> <p>This kind of friction reduction is a game changer for productivity.</p> <p>Another reason MCP is pivotal: <strong>It enables vendor-agnostic development</strong>. You’re not locking into one AI provider’s ecosystem or a single toolchain. Since MCP is an open standard, any AI client (Claude, other LLM chatbots, or open source LLMs) can use any MCP server. This means developers and companies can mix and match—e.g., use Anthropic’s Claude for some tasks, switch to an open source LLM later—and their <strong>MCP-based integrations remain intact</strong>. That flexibility derisks adopting AI: You’re not writing one-off code for, say, OpenAI’s plug-in format that becomes useless elsewhere. It’s more like building a standard API that any future AI can call. In fact, we’re already seeing multiple IDEs and tools embrace MCP (Cursor, Windsurf, Cline, the Claude desktop app, etc.), and even model-agnostic frameworks like LangChain provide adapters for MCP. This momentum suggests MCP could become the <strong>de facto interoperability layer</strong> for AI agents. As one observer put it, what’s to stop MCP from evolving into a “true interoperability layer for agents” connecting everything?</p> <p>MCP is also a boon for tool developers. If you’re building a new developer tool today, making it MCP-capable vastly increases its power. Instead of only having a GUI or API that humans use, you get an <strong>AI interface “for free.”</strong> This idea has led to the concept of “<strong>MCP-first development</strong>,” where you build the MCP server for your app <em>before</em> or alongside the GUI. By doing so, you ensure from day one that AI can drive your app. Early adopters have found this extremely beneficial. “With MCP, we can test complex game development workflows by simply asking Claude to execute them,” says Miguel Tomas, creator of the Unity MCP server. This not only speeds up testing (the AI can rapidly try sequences of actions in Unity) but also indicates a future where <strong>AI is a first-class user</strong> of software, not an afterthought.</p> <p>Finally, consider the <strong>efficiency and capability boost</strong> for AI agents. Before MCP, if an AI agent needed some info from a third-party app, it was stuck unless a developer had foreseen that need and built a custom plug-in. Now, as the ecosystem of MCP servers grows, AI agents can tackle a much wider array of tasks out of the box by leveraging existing servers. Need to schedule a meeting? There might be a Google Calendar MCP. Analyze customer tickets? Perhaps a Zendesk MCP. The <strong>barrier to multistep, multisystem automation drops</strong> dramatically. This is why many in the AI community are excited: MCP could unlock a new wave of <strong>AI orchestration</strong> across our tools. We’re already seeing demos where a single AI agent moves fluidly from emailing someone to updating a spreadsheet to creating a Jira ticket, all through MCP connectors. The potential to <strong>compose these actions</strong> into sophisticated workflows (with the AI handling the logic) could usher in a “new era” of intelligent automation, as <a href="https://x.com/sidahuj" target="_blank" rel="noreferrer noopener">Siddharth Ahuja</a> <a href="https://www.linkedin.com/feed/update/urn:li:activity:7307611669445128192/" target="_blank" rel="noreferrer noopener">described</a> after connecting Blender via MCP.</p> <p>In summary, MCP matters because it turns the dream of a <strong>universal AI assistant for developers</strong> into a practical reality. It’s the missing piece that makes our tools <strong>context aware and interoperable</strong> with AI, with immediate productivity wins (less manual glue work) and strategic advantages (future-proof, flexible integrations). The next sections will make this concrete by walking through some eye-opening demos and use cases made possible by MCP.</p> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/mcp-what-it-is-and-why-it-matters-part-2/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
</item>
<item>
<title>5 Skills Kids (and Adults) Need in an AI World</title>
<link>https://www.oreilly.com/radar/5-skills-kids-and-adults-need-in-an-ai-world/</link>
<comments>https://www.oreilly.com/radar/5-skills-kids-and-adults-need-in-an-ai-world/#respond</comments>
<pubDate>Thu, 22 May 2025 10:37:47 +0000</pubDate>
<dc:creator>
<![CDATA[ Raffi Krikorian ]]>
</dc:creator>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Research ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16755</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/creative-light-abstract-night-texture-dark-1092382-pxhere_crop-ec278ad7ebdac1d6235975d3ac905cba.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ Hint: Coding Isn&#039;t One of Them ]]>
</custom:subtitle>
<description>
<![CDATA[ Last week, I found myself hunched over my laptop at 10 p.m. (hey, that&#8217;s late for me!), wrestling with a coding problem. After hours of frustration, I stepped away and made a cup of tea. When I returned, I did what any self-respecting technologist in 2025 would do: I backtracked, reformulated my question, and asked [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>Last week, I found myself hunched over my laptop at 10 p.m. (hey, that&#8217;s late for me!), wrestling with a coding problem. After hours of frustration, I stepped away and made a cup of tea. When I returned, I did what any self-respecting technologist in 2025 would do: I backtracked, reformulated my question, and asked ChatGPT for help.</p> <p>I’m constantly asked questions like &#8220;Should my kids learn to code?&#8221; and &#8220;What skills do they actually need in this AI world?&#8221; I wonder about this too. I mean, if AI can now write code better than most humans, should we still be teaching kids to do it? How do we prepare them for the future, especially as things are moving so quickly?</p> <p>Perhaps counterintuitively, this AI revolution might make a liberal arts education more valuable. A poetry major learns how to express humanity. A historian learns lessons from the past. A philosophy student learns to question assumptions and ethical frameworks. These timeless human skills become even more crucial as AI handles the technical heavy lifting. With these foundational abilities to understand and express the human condition, what&#8217;s possible with creativity becomes boundless.</p> <h2 class="wp-block-heading"><strong>The End of Coding Is the Beginning of Problem-Solving</strong></h2> <p>As AI starts writing code, we&#8217;re entering what my friend Tim O&#8217;Reilly calls &#8220;the end of programming as we know it.&#8221; We&#8217;ve gone from punch cards to assembly language to C, Python, and JavaScript—and now we&#8217;re just telling computers what to do in plain language. That shift opens the door for more people to shape technology. The future isn&#8217;t about knowing code; it&#8217;s about knowing what to build and why.</p> <p>Stanford researchers, including Noah Goodman (who&#8217;s both a computer scientist and a psychologist studying human cognition), recently published a <a href="https://arxiv.org/pdf/2503.01307" target="_blank" rel="noreferrer noopener">fascinating paper</a> examining how different AI systems approach problem-solving.</p> <p>What makes Goodman’s perspective so valuable is his dual expertise in how minds, both human and artificial, work. His paper shows that the thinking patterns that make certain AI systems more successful mirror those of effective human problem-solvers: The most successful systems verify their work, backtrack when stuck, break big problems into manageable subgoals, and work backward from desired outcomes.</p> <p>It’s a profound discovery: The skills that make humans effective problem-solvers will remain valuable regardless of how AI evolves. It made me realize that these cognitive behaviors—not coding syntax—are what we should be nurturing in our children.</p> <h2 class="wp-block-heading"><strong>Five Essential Skills Kids Need (More than Coding)</strong></h2> <p>I&#8217;m not saying we shouldn&#8217;t teach kids to code. It’s a useful skill. But these are the five true foundations that will serve them regardless of how technology evolves.</p> <h3 class="wp-block-heading"><strong>1. Loving the journey, not just the destination</strong></h3> <p>When homework seems impossible or a LEGO structure collapses for the fifth time, it&#8217;s easy for kids to get discouraged. But teaching them that setbacks are learning opportunities builds the bounce-back ability they&#8217;ll need in a rapidly changing world. The capacity to absorb genuine setbacks and continue forward—discovering something new even when they don&#8217;t reach their initial goal—might be the single most important skill we can nurture in our kids.</p> <p>Developing a love of learning helps them to see tough problems as interesting puzzles rather than scary roadblocks. This doesn’t just apply to academic subjects. Genuine curiosity about the world prepares children to adapt continuously. The most successful people I know aren&#8217;t those who memorized the most facts or mastered one specific skill; they&#8217;re the ones who stayed curious and kept going through constant change.</p> <p>We often talk about intrinsic motivation as a prerequisite for learning, but it&#8217;s also a muscle you build through the learning process. As children tackle challenges and experience the satisfaction of overcoming them, they&#8217;re not just solving problems; they&#8217;re developing the motivation to tackle the next one.</p> <h3 class="wp-block-heading"><strong>2. Being a question-asker, not just an answer-getter</strong></h3> <p><em>When you&#8217;re a student, you&#8217;re judged by how well you answer questions.…But in life, you&#8217;re judged by how good your questions are.—Robert Langer, MIT Professor and Cofounder of Moderna</em></p> <p>Anyone can ask AI for answers. Those who ask thoughtful questions will get the most from it. Good questions stem from understanding what you don&#8217;t know, being clear about what you&#8217;re really looking for, and framing them in a way that leads to meaningful answers.</p> <p>One of the most powerful metaskills we can help children develop is self-awareness about their own learning style. Some are project-based learners who need to build something in order to understand it. Others learn through conversation, writing, visualization, or teaching others. When a child discovers how their brain works best, they can approach any new subject through the lens that works for them, turning what might have been a struggle into a natural process.</p> <p>When a child asks, &#8220;Why is the sky blue?,&#8221; they&#8217;re doing something powerful: noticing patterns, questioning what others take for granted, and seeking deeper understanding. Children who learn to ask good questions will direct the world rather than be directed by it. They&#8217;ll know how to break big problems into solvable pieces—an approach that works in any field.</p> <h3 class="wp-block-heading"><strong>3. Trying, failing, and trying differently</strong></h3> <p>When solving problems, scientists don&#8217;t move forward in a straight line. They make guesses, test them, and often discover they were wrong. Then they use that information to make better guesses. This try-learn-adjust loop is something all successful problem-solvers use, whether they&#8217;re fixing code or figuring out life.</p> <p>When something doesn&#8217;t work as expected—including an AI-generated answer—kids need to figure out what went wrong and then try different approaches. This means getting comfortable with saying things like &#8220;Let me try a different way&#8221; or &#8220;That didn&#8217;t work because&#8230;&#8221;</p> <p>Whether they&#8217;re troubleshooting a device or navigating everyday challenges, this mindset helps them approach problems with confidence rather than giving up.</p> <h3 class="wp-block-heading"><strong>4. Seeing the whole picture</strong></h3> <p>The biggest challenges we currently face, from climate change to healthcare, require understanding how different pieces connect and influence each other. This &#8220;big-picture thinking&#8221; applies equally to everyday situations, such as understanding why a classroom gets noisy or why a family budget doesn&#8217;t balance.</p> <p>This mindset is about spotting patterns and understanding how changing one thing affects everything else. It helps us anticipate unintended consequences and create solutions that actually work.</p> <p>When we teach kids to see connections rather than isolated facts, we prepare them to tackle problems that AI alone can&#8217;t solve. They become directors rather than followers, able to combine human needs with technological possibilities.</p> <h3 class="wp-block-heading"><strong>5. Walking in others’ shoes</strong></h3> <p>In <a href="https://www.chicagotribune.com/2025/03/13/opinion-government-efficiency-technology-empathy/" target="_blank" rel="noreferrer noopener">my recent op-ed for the <em>Chicago Tribune</em></a>, I argued that efficiency and empathy aren&#8217;t opposing forces. They need each other. This principle is especially important as we raise the next generation.</p> <p>Technology without human understanding leads to solutions that might look good on paper but forget the real people they&#8217;re meant to help. I&#8217;ve seen this firsthand in government systems that process people efficiently but fail to recognize their dignity and unique situations.</p> <p>Children who develop deep empathy will create technologies that truly serve humanity rather than just serving statistics. They&#8217;ll ask not only &#8220;Can we build this?&#8221; but &#8220;<em>Should</em> we build this, and who will it help or harm?&#8221; They&#8217;ll remember that behind every data point is a human story, and that the most meaningful innovations are those that strengthen our connections to one another.</p> <h2 class="wp-block-heading"><strong>The Real Future: Amplifying Human Creativity</strong></h2> <p>These five skills converge in what I see as the most exciting aspect of our AI-augmented future: democratized creation. As more people gain the ability to shape technology, even without traditional coding skills, we&#8217;ll see an explosion of local, purpose-driven solutions.</p> <p>As I recently <a href="https://technicallyoptimistic.substack.com/p/the-real-future-of-ai" target="_blank" rel="noreferrer noopener">wrote</a>, I helped put together <a href="https://tumo.ai/teens" target="_blank" rel="noreferrer noopener">ai/teens</a>, the first global AI conference for and by teens. I wanted to learn from the first AI-native generation, which intuitively understands technology&#8217;s potential in ways many adults don&#8217;t.</p> <p>Imagine a world where young people not only use technology but actively shape it to solve problems in their communities, designing accessibility tools for friends with disabilities, creating platforms that connect local resources with those who need them, or building educational experiences tailored to different learning styles.</p> <p>This future isn&#8217;t about AI replacing human creativity; it&#8217;s about amplifying it, making it possible for more people to bring their unique perspectives and solutions to life.</p> <h2 class="wp-block-heading"><strong>Let&#8217;s Build This Future Together!</strong></h2> <p>The beauty of this approach—focusing on resilience, questioning, adaptation, systems thinking, and empathy—is that it works regardless of how technology evolves. The most technologically advanced future still needs people who can embrace challenges, ask meaningful questions, learn continuously, see connections, and understand each other.</p> <p>In many ways, we&#8217;re returning to the ideal of a classical education for the AI age. These skills form a modern trivium—not grammar, logic, and rhetoric but perhaps curiosity, creativity, and compassion—foundational abilities that unlock all other learning and doing.</p> <p>Let&#8217;s work on this as a community! I&#8217;m crowdsourcing ideas, activities, and approaches that help develop these essential skills. What other skills do you think we should focus on? I’m eager to learn with all of you.<br></p> <p></p> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/5-skills-kids-and-adults-need-in-an-ai-world/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
</item>
<item>
<title>Securing AI: Building with Guardrails Before Acceleration</title>
<link>https://www.oreilly.com/radar/securing-ai-building-with-guardrails-before-acceleration/</link>
<comments>https://www.oreilly.com/radar/securing-ai-building-with-guardrails-before-acceleration/#respond</comments>
<pubDate>Tue, 20 May 2025 10:28:46 +0000</pubDate>
<dc:creator>
<![CDATA[ Jennifer Pollock ]]>
</dc:creator>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Security ]]>
</category>
<category>
<![CDATA[ Signals ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16698</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/hacker-1944688_crop-b34a76e3cab9c07c5900b706c70a12c3.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ Protect Systems from the Foundation Up—Before Velocity Outpaces Safety ]]>
</custom:subtitle>
<description>
<![CDATA[ It’s been less than three years since OpenAI released ChatGPT, setting off the GenAI boom. But in that short time, software development has transformed: code-complete assistants evolved into chat-based “vibe coding,” and now we&#8217;re entering the agent era, where developers may soon be managing fleets of autonomous coders (if Steve Yegge’s predictions are correct). Writing [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>It’s been less than three years since OpenAI released ChatGPT, setting off the GenAI boom. But in that short time, software development has transformed: code-complete assistants evolved into chat-based “vibe coding,” and now we&#8217;re entering the agent era, where developers may soon be managing fleets of autonomous coders (<a href="https://sourcegraph.com/blog/revenge-of-the-junior-developer" target="_blank" rel="noreferrer noopener">if Steve Yegge’s predictions are correct</a>). Writing code has never been easier, but securing it hasn’t kept pace. Bad actors have wasted no time targeting vulnerabilities in AI-generated code. For AI-native organizations, lagging security isn’t just a liability—it’s an existential risk. So the question isn’t just “Can we build?” It’s “Can we build safely?”</p> <p>Security conversations still tend to center around the model. In fact, a new working paper from the AI Disclosures Project finds that corporate AI labs focus most of their research on “<a href="https://asimovaddendum.substack.com/p/we-analyzed-over-9000-generative" target="_blank" rel="noreferrer noopener">pre-deployment, pre-market, concerns such as alignment, benchmarking, and interpretability</a>.”<sup>1</sup> Meanwhile, the real threat surface emerges <em>after</em> deployment. That’s when GenAI apps are vulnerable to prompt injection, data poisoning, agent memory manipulation, and context leakage—today’s version of SQL injection. Unfortunately, many GenAI apps have minimal input sanitization or system-level validation. That has to change. As Steve Wilson, author of <a href="https://learning.oreilly.com/library/view/the-developers-playbook/9781098162191/" target="_blank" rel="noreferrer noopener"><em>The Developer&#8217;s Playbook for Large Language Model Security</em></a>, warns, “Without a deep dive into the murky waters of LLM security risks and how to navigate them, we’re not just risking minor glitches; we’re courting major catastrophes.”</p> <p>And if you’re “<a href="https://x.com/karpathy/status/1886192184808149383?lang=en" target="_blank" rel="noreferrer noopener">fully giv[ing] in to the vibes</a>” and running AI-generated code you haven’t reviewed, you’re compounding the problem. When insecure defaults get baked in, they’re difficult to detect—and even harder to unwind at scale. You have no idea <a href="https://www.theregister.com/2025/04/12/ai_code_suggestions_sabotage_supply_chain/" target="_blank" rel="noreferrer noopener">what vulnerabilities</a> may be creeping in.</p> <p>Security may be “everyone’s responsibility,” but in AI systems, not everyone’s responsibilities are the same. Model providers should ensure their systems resist prompt-based manipulation, sanitize training data, and mitigate harmful outputs. But most AI risk emerges once those models are deployed in live systems. Infrastructure teams must lock down data authentication and interagent access using zero trust principles. App developers hold the frontline, applying traditional secure-by-design principles in entirely new interaction models.</p> <p><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming" target="_blank" rel="noreferrer noopener">Microsoft’s recent work on AI red teaming</a> shows how guardrail strategies should be adapted (in some cases radically so) depending on use case: What works for a coding assistant might fail in an autonomous sales agent, for instance. The shared stack doesn’t imply shared responsibility; it requires clearly delineated roles and proactive security ownership at every layer.</p> <p>Right now, we don’t know what we don’t know about AI models—and as Bruce Schneier recently pointed out (in response to new research on <a href="https://arxiv.org/html/2502.17424v1" target="_blank" rel="noreferrer noopener">emergent misalignment</a>): “The emergent properties of LLMs <a href="https://www.schneier.com/blog/archives/2025/02/emergent-misalignment-in-llms.html" target="_blank" rel="noreferrer noopener">are so, so weird</a>.” It turns out, models tuned on insecure prompts develop other misaligned outputs. What else might we be missing? One thing is clear: Inexperienced coders are introducing vulnerabilities as they vibe, whether those security risks turn up in the code itself or in biased or otherwise harmful outputs. And they may not catch, or even be aware of, the dangers—new developers often fail to test for adversarial inputs or agentic recursion. Vibe coding may help you quickly spin up a project, but as Steve Yegge warns, “<a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/genai_in_the_real_world_with_steve_yegge_1.mp3#t=438" target="_blank" rel="noreferrer noopener">You can’t trust anything. You have to validate and verify</a>.” (Addy Osmani puts it a little differently: “<a href="https://addyo.substack.com/p/vibe-coding-is-not-an-excuse-for" target="_blank" rel="noreferrer noopener">Vibe Coding is not an excuse for low-quality work</a>.”) Without an intentional focus on security, your fate may be “Prototype today, exploit tomorrow.”</p> <p>The next evolutionary step—agent-to-agent coordination—only widens the threat surface. Anthropic’s <a href="https://www.anthropic.com/news/model-context-protocol" target="_blank" rel="noreferrer noopener">Model Context Protocol</a> and Google’s <a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/" target="_blank" rel="noreferrer noopener">Agent2Agent</a> enable agents to act across multiple tools and data sources, but this interoperability can deepen vulnerabilities if assumed secure by default. Layering A2A into existing stacks without red teams or zero trust principles is like connecting microservices without API gateways. These platforms must be designed with security-first networking, permissions, and observability baked in. The good news: Fundamental skills still work. Layered defenses, red teaming, least-privilege permissions, and secure model interfaces are still your best tools. The guardrails aren’t new. They’re just more essential than ever.</p> <p>O’Reilly founder Tim O’Reilly is fond of quoting designer Edwin Schlossberg, who noted that “the skill of writing is to create a context in which other people can think.” In the age of AI, those responsible for keeping systems safe must broaden the context within which we <em>all</em> think about security. The task is more important—and more complex—than ever. Don’t wait until you’re moving fast to think about guardrails. Build them in first, then build securely from there.</p> <hr class="wp-block-separator has-alpha-channel-opacity"/> <h3 class="wp-block-heading">Footnotes</h3> <ol class="wp-block-list"> <li>Ilan Strauss, Isobel Moure, Tim O’Reilly, and Sruly Rosenblat, “<a href="https://ssrc-static.s3.us-east-1.amazonaws.com/Real-World-Gaps-in-AI-Governance-Research-Strauss-Moore-OReilly-Rosenblat_SSRC_04302025.pdf" target="_blank" rel="noreferrer noopener">Real-World Gaps in AI Governance Research</a>,” The AI Disclosures Project, 2024. The AI Disclosures Project is co-led by O’Reilly Media founder Tim O’Reilly and economist Ilan Strauss.</li> </ol> <hr class="wp-block-separator has-alpha-channel-opacity"/> <p class="has-black-color has-cyan-bluish-gray-background-color has-text-color has-background has-link-color wp-elements-38df52a59b068858d9c0f5b9189d6336"><em>Join Tim O’Reilly and Steve Wilson on June 3 for Building Secure Code in the Age of Vibe Coding—it&#8217;s free and open to all. After an introductory conversation with Tim on how AI-assisted coding (and vibe coding in particular) introduces new classes of security vulnerabilities, Steve will respond to questions from attendees, giving you a chance to better understand how his insights apply to your own situation and experiences. <a href="https://www.oreilly.com/live/building-secure-code-in-the-age-of-vibe-coding.html" target="_blank" rel="noreferrer noopener">Register now to save your spot</a>.</em></p> <p></p> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/securing-ai-building-with-guardrails-before-acceleration/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
<enclosure url="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/genai_in_the_real_world_with_steve_yegge_1.mp3" length="38412372" type="audio/mpeg"/>
</item>
<item>
<title>An Architecture of Participation for AI?</title>
<link>https://www.oreilly.com/radar/an-architecture-of-participation-for-ai/</link>
<comments>https://www.oreilly.com/radar/an-architecture-of-participation-for-ai/#respond</comments>
<pubDate>Mon, 19 May 2025 17:13:47 +0000</pubDate>
<dc:creator>
<![CDATA[ Tim O’Reilly ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Commentary ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16719</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_satya_nadella.jpg" medium="image" type="image/jpeg"/>
<description>
<![CDATA[ About six weeks ago, I sent an email to Satya Nadella complaining about the monolithic winner-takes-all architecture that Silicon Valley seems to envision for AI, contrasting it with “the architecture of participation” that had driven previous technology revolutions, most notably the internet and open source software. I suspected that Satya might be sympathetic because of [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>About six weeks ago, I sent an email to Satya Nadella complaining about the monolithic winner-takes-all architecture that Silicon Valley seems to envision for AI, contrasting it with “the architecture of participation” that had driven previous technology revolutions, most notably the internet and open source software. I suspected that Satya might be sympathetic because of <a href="https://www.linkedin.com/pulse/conversation-satya-nadella-his-new-book-hit-refresh-tim-o-reilly" target="_blank" rel="noreferrer noopener">past conversations we’d had</a> when his book <a href="https://www.amazon.com/Hit-Refresh-Rediscover-Microsofts-Everyone-ebook/dp/B01HOT5SQA" target="_blank" rel="noreferrer noopener"><em>Hit Refresh</em></a> was published in 2017.</p> <p>I made the case that we need an architecture for the AI industry that enables cooperating AIs, that isn&#8217;t a winner-takes-all market, and that doesn&#8217;t make existing companies in every industry simply the colonial domains of extractive AI conquerors, which seems to be the Silicon Valley vision.</p> <p>Little did I know that Microsoft already had something in the works that is a demonstration of what I am hoping for. It’s called NLWeb (Natural Language Web), and it’s being announced today. Satya offered O’Reilly the chance to be part of the rollout, and we jumped at it.</p> <h2 class="wp-block-heading">Embracing the Early Stage of Innovation</h2> <p>My ideas are rooted in a notion about how technology markets evolve. We have lived through three eras in computing. Each began with distributed innovation, went through a period of fierce competition, and ended with monopolistic gatekeepers. In the first age (mainframes), it was IBM, in the second (PCs) Microsoft, and in the third (internet and mobile) the oligopoly of Google, Amazon, Meta, and Apple.</p> <p>The mistake that everyone makes is a rush to crown the new monopolist at the start of what is essentially a wide-open field at the beginning of a new disruptive market. And they envision that monopoly largely as a replacement for what went before, rather than realizing that the paradigm has changed. When the personal computer challenged IBM’s hardware-based monopoly, companies raced to become the dominant personal computer hardware company. Microsoft won because it realized that software, not hardware, was the new source of competitive advantage.</p> <p>The story repeated itself at the beginning of the internet era. Marc Andreessen’s Netscape sought to replace Microsoft as a dominant software platform, except for the internet rather than the PC. AOL realized that content and community, not software, was going to be a source of competitive advantage on the internet, but they made the same mistake of assuming the end game of consolidated monopoly rather than embracing the early stage of distributed innovation.</p> <figure class="wp-block-image size-large"><img fetchpriority="high" decoding="async" width="1048" height="590" src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators-1048x590.jpg" alt="" class="wp-image-16741" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators-1048x590.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators-300x169.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators-768x432.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators-1536x864.jpg 1536w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_collaborators.jpg 1678w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption class="wp-element-caption">Microsoft CTO Kevin Scott announces NLWeb at Microsoft Build 2025.</figcaption></figure> <p>So here we are at the beginning of the fourth age, the age of AI, and once again, everyone is rushing to crown the new king. So much of the chatter is whether OpenAI or one of its rivals will be the next Google, when it looks to me that they are more likely the next Netscape or the next AOL. DeepSeek has thrown a bomb into the coronation parade, but we haven&#8217;t yet fully realized the depth of the reset, or conceptualized what comes next. <em>That is typically figured out through a period of distributed innovation.</em></p> <h2 class="wp-block-heading">We Need an Architecture of Participation for AI</h2> <p>The term “<a href="https://www.oreilly.com/pub/a/tim/articles/architecture_of_participation.html" target="_blank" rel="noreferrer noopener">the architecture of participation</a>” originally came to me as an explanation of why Unix had succeeded as a collaborative project despite its proprietary license while other projects failed despite having open source licenses. Unix was designed as a small operating system kernel supporting layers of utilities and applications that could come from anyone, as long as they followed the same rules. Complex behaviors could be assembled by passing information between small programs using standard data formats. It was a protocol-centric view of how complex software systems should be built, and how they could evolve collaboratively. Linux, of course, began as a re-implementation of Unix, and it was the architecture of participation that it inherited, as much as the license and the community, that was the foundation of its success. The internet was also developed as a distributed, protocol-based system.</p> <p>That concept ran through my web advocacy in the early ’90s, open source advocacy in the late ’90s, and Web 2.0 in the aughts. Participatory markets are innovative markets; prematurely consolidated markets, not so much. The barriers to entry in the early PC market were very low, entrepreneurship high. Ditto for the Web, ditto for open source software and for Web 2.0.&nbsp; For late Silicon Valley, <a href="https://www.linkedin.com/posts/timo3_the-fundamental-problem-with-silicon-valley-activity-6498637102643843072-9duJ?trk=public_profile_like_view" target="_blank" rel="noreferrer noopener">fixated on premature monopolization via “blitzscaling”</a> (think Uber, Lyft, and WeWork as examples, and now OpenAI and Anthropic), not so much. It’s become <a href="https://www.oreilly.com/radar/ai-has-an-uber-problem/" target="_blank" rel="noreferrer noopener">a kind of central planning</a>. A small cadre of deep-pocketed investors pick the winners early on and try to drown out competition with massive amounts of capital rather than allowing the experimentation and competition that allows for the discovery of true product-market fit.</p> <p>And I don’t think we have that product-market fit for AI yet. Product-market fit isn’t just getting lots of users. It’s also finding business models that pay the costs of those services, and that create value for more than the centralized platform. The problem with premature consolidation is that it narrows the focus to the business model of the platform, often at the expense of its ecosystem of developers.</p> <p>As Bill Gates famously <a href="https://stratechery.com/2018/the-bill-gates-line/" target="_blank" rel="noreferrer noopener">told Chamath Palihapitiya</a> when he was running the nascent (and ultimately failed) Facebook developer platform, “This isn’t a platform. A platform is when the economic value of everybody that uses it exceeds the value of the company that creates it. Then it’s a platform.” To be clear, that is not just value to end users. It’s value to developers and entrepreneurs. And that means the opportunity to profit from their innovations, not to have that value immediately harvested by a dominant gatekeeper.</p> <p>Now of course, Sam Altman talks about creating value for developers. In a recent appearance at Sequoia Capital’s AI Ascent event, <a href="https://www.windowscentral.com/software-apps/openai-subscription-based-operating-system-on-chatgpt">he said</a> his hope is to create “like just an unbelievable amount of wealth creation in the world and other people to build on that.” But he uses the language of “an operating system” that others build on top of (and pay OpenAI for the use of) rather than a shared infrastructure co-created by an ecosystem of developers.</p> <p>That’s why I’ve been rooting for something different. A world where specialized content providers can build AI interfaces to their own content rather than having it sucked up by AI model builders who offer up services based on it to their own users. A world where application developers can offer new kinds of services that enable others in a cooperative cascade.</p> <h2 class="wp-block-heading">We&#8217;re Just Getting Started</h2> <p>Anthropic’s Model Context Protocol, an open standard for connecting AI agents and assistants to data sources, is the first step toward a protocol-centric vision of cooperating AIs. It has generated a lot of well-deserved enthusiasm. Google’s A2A takes that further with a vision of how AI agents might cooperate. NLWeb adds to that an easy way for internet content sites to join the party, offering both a conversational front end to their content and an MCP server so that it is accessible to agents.</p> <p>This is all going to take years to get right. But because it’s a protocol-centric rather than a platform-centric vision, solutions can come from everywhere, not just from a dominant monopolist.</p> <p>Every new wave of computing has also had a new user interface paradigm. In the mainframe era, it was the teletype terminal; for the PC, the Graphical User Interface; for the internet, the web’s document-centric interface; for mobile, touch screens. For AI (for now at least), it appears to be conversational interfaces.</p> <p>Companies such as Salesforce and Bret Taylor’s Sierra are betting on conversational agents that are front ends to companies, their services, and their business processes, in the same way that their website or mobile app is today. Others are betting on client-side agents that will access remote sites, but often by calling APIs or even performing the equivalent of screen scraping. MCP, A2A, and other agent protocols point to a richer interaction layer made up of cooperating AIs, able to connect to <em>any site</em> offering AI services, not just via API calls to a dominant AI platform.</p> <p>All companies need at least a start on an AI frontend today. There’s a fabulous line from C. S. Lewis’s novel <a href="https://en.wikipedia.org/wiki/Till_We_Have_Faces" target="_blank" rel="noreferrer noopener"><em>Till We Have Faces</em></a>:<em> </em>“We cannot see the gods face to face until we have faces.” Right now, some companies are able to offer an AI face to their users, but most do not. NLWeb is a chance for every company to have an AI interface (or simply “face”) for not just their human users but any bot that chooses to visit.</p> <figure class="wp-block-image size-large"><img decoding="async" width="1048" height="589" src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_oreilly_prototype-1048x589.jpg" alt="" class="wp-image-16742" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_oreilly_prototype-1048x589.jpg 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_oreilly_prototype-300x169.jpg 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_oreilly_prototype-768x432.jpg 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/05/nlweb_oreilly_prototype.jpg 1254w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption class="wp-element-caption">Microsoft&#8217;s Kevin Scott shares a glimpse of O&#8217;Reilly&#8217;s forthcoming NLWeb demo.</figcaption></figure> <p>NLWeb is fully compatible with MCP and offers existing websites a simple mechanism to add AI search and other services to an existing web frontend. We put together our demo AI search frontend for O’Reilly in a few days. We’ll be rolling it out to the public soon.</p> <p><a href="https://aka.ms/AAw2etx">Give it a try</a></p> ]]>
</content:encoded>
<wfw:commentRss>https://www.oreilly.com/radar/an-architecture-of-participation-for-ai/feed/</wfw:commentRss>
<slash:comments>0</slash:comments>
</item>
<item>
<title>Takeaways from Coding with AI</title>
<link>https://www.oreilly.com/radar/takeaways-from-coding-with-ai/</link>
<pubDate>Wed, 14 May 2025 09:56:49 +0000</pubDate>
<dc:creator>
<![CDATA[ Tim O’Reilly ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Commentary ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16664</guid>
<description>
<![CDATA[ I thought I’d offer a few takeaways and reflections based on last week’s first AI Codecon virtual conference, Coding with AI: The End of Software Development as We Know It. I’m also going to include a few short video excerpts from the event. If you registered for Coding with AI or if you’re an existing [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>I thought I’d offer a few takeaways and reflections based on last week’s first AI Codecon virtual conference, <a href="https://www.oreilly.com/CodingwithAI/" target="_blank" rel="noreferrer noopener">Coding with AI: The End of Software Development as We Know It</a>. I’m also going to include a few short video excerpts from the event. If you registered for Coding with AI or if you’re an existing O’Reilly subscriber, you can watch or rewatch the whole thing on the O’Reilly learning platform. If you aren’t a subscriber yet, it’s easy to <a href="https://www.oreilly.com/start-trial/" target="_blank" rel="noreferrer noopener">start a free trial</a>. We’ll also be posting additional excerpts on the <a href="https://www.youtube.com/oreillymedia" target="_blank" rel="noreferrer noopener">O’Reilly YouTube channel</a> in the next few weeks.</p> <p>But on to the promised takeaways.</p> <p>First off, Harper Reed is a mad genius who made everyone’s head explode. (Camille Fournier apparently has joked that Harper has rotted his brain with AI, and Harper actually agreed.) Harper discussed his design process in a talk that you might want to run at half speed. His greenfield workflow is to start with an idea. Give your idea to a chat model and have it ask you questions with yes/no answers. Have it extract all the ideas. That becomes your spec or PRD. Use the spec as input to a reasoning model and have it generate a plan; then feed that plan into a different reasoning model and have it generate prompts for code generation for both the application and tests. He’s having a wild time.</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/h2giTZogX0M?si=lLTFUZSe2HC8FKpD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> <p></p> <p><a href="https://agilemanifesto.org/" target="_blank" rel="noreferrer noopener"><em>Agile Manifesto</em></a> coauthor Kent Beck was also on Team Enthusiasm. He told us that augmented coding with AI was “the most fun I’ve ever had,” and said that it “reawakened the joy of programming.” Nikola Balic agreed: “As Kent said, it just brought the joy of writing code, the joy of programming, it brought it back. So I&#8217;m now generating more code than ever. I have, like, a million lines of code in the last month. I&#8217;m playing with stuff that I never played with before. And I&#8217;m just spending an obscene amount of tokens.” But in the future, “I think that we won&#8217;t write code anymore. We will nurture it. This is a vision. I&#8217;m sure that many of you will disagree but let&#8217;s look years in the future and how everything will change. I think that we are more going toward intention-driven programming.”</p> <p>Others, like Chelsea Troy, Chip Huyen, swyx, Birgitta Böckeler, and Gergely Orosz weren’t so sure. Don’t get me wrong. They think that there’s a ton of amazing stuff to do and learn. But there’s also a lot of hype and loose thinking. And while there will be a lot of change, a lot of existing skills will remain important.</p> <p>Here’s Chelsea’s critique of the <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566" target="_blank" rel="noreferrer noopener">recent paper that claimed a 26% productivity increase</a> for developers using generative AI.</p> <iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/bg4z70cOOF4?si=rTz_EMfXgWv7zA0e" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> <p></p> <p>If Chelsea will do a sermon every week in the Church of Don’t Believe Everything You Read that consists of her showing off various papers and giving her dry and insightful perspective on how to think about them more clearly, I am so there.</p> <p>I was a bit surprised by how skeptical Chip Huyen and swyx were about A2A. They really schooled me on the notion that the future of agents is in direct AI-to-AI interactions. I’ve been of the opinion that having an AI agent work the user-facing interface of a remote website is a throwback to screen scraping—surely a transitional stage—and while calling an API will be the best way to handle a deterministic process like payment, there will be a whole lot of other activities, like taste matching, that are ideal for LLM to LLM. When I think about AI shopping for example, I imagine an agent that has learned and remembered my tastes and preferences and specific goals communicating with an agent that knows and understands the inventory of a merchant. But swyx and Chip weren’t buying it, at least not now. They think that’s a long way off, given the current state of AI engineering. I was glad to have them bring me back to earth.</p> <p>(For what it’s worth, Gabriela de Queiroz, director of AI at Microsoft, agrees. On <a href="https://www.oreilly.com/radar/podcast/generative-ai-in-the-real-world-the-startup-opportunity-with-gabriela-de-queiroz/" target="_blank" rel="noreferrer noopener">her episode</a> of O’Reilly’s <em>Generative AI in the Real World</em> podcast, she said, “If you think we&#8217;re close to AGI, try building an agent, and you&#8217;ll see how far we are from AGI.”)</p> <p>Angie Jones, on the other hand, was pretty excited about agents in her <a href="https://en.wikipedia.org/wiki/Lightning_talk" target="_blank" rel="noreferrer noopener">lightning talk</a> about how MCP is bringing the “mashup” era back to life. I was struck in particular by Angie’s comments about MCP as a kind of universal adapter, which abstracts away the underlying details of APIs, tools, and data sources. That was a powerful echo of Microsoft’s platform dominance in the Windows era, which in many ways began with the Win32 API, which abstracted away all the underlying hardware such that application writers no longer had to write drivers for disk drives, printers, screens, or communications ports. I’d call that a power move by Anthropic, except for the blessing that they introduced MCP as an open standard. Good for them!</p> <p>Birgitta Böckeler talked frankly about how LLMs helped reduce cognitive load and helped think through a design. But much of our daily work is a poor fit for AI: large legacy codebases where we change more code than we create, antiquated technology stacks, poor feedback loops. We still need code that is simple and modular—that’s easier for LLMs to understand, as well as humans. We still need good feedback loops that show us whether code is working (echoing Harper). We still need logical, analytical, critical thinking about problem solving. At the end, she summarized both poles of the conference, saying we need cultures that reward both experimentation and skepticism.</p> <p>Gergely Orosz weighed in on the continued importance of software engineering. He talked briefly about books he was reading, starting with Chip Huyen’s <a href="https://www.oreilly.com/library/view/ai-engineering/9781098166298/" target="_blank" rel="noreferrer noopener"><em>AI Engineering</em></a>, but perhaps the more important point came a bit later: He held up several software engineering classics, including <a href="https://www.oreilly.com/library/view/mythical-man-month-the/0201835959/" target="_blank" rel="noreferrer noopener"><em>The Mythical Man-Month</em></a> and <a href="https://www.oreilly.com/library/view/code-complete-2nd/0735619670/" target="_blank" rel="noreferrer noopener"><em>Code Complete</em></a>. These books are decades old, Gergely noted, but even with 50 years of tool development, the problems they describe are still with us. AI isn’t likely to change that.</p> <p>In this regard, I was struck by Camille Fournier’s assertion that managers love to see their senior developers using AI tools, because they have the skills and judgment to get the most out of it, but often want to take it away from junior developers who can use it too uncritically. Addy Osmani expressed the concern that basic skills (“muscle memory”) would degrade, both for junior and senior software developers. (Juniors may never develop those skills in the first place.) Addy’s comment was echoed by many others. Whatever the future of computing holds, we still need to know how to analyze a problem, how to think about data and data structures, how to design, and how to debug.</p> <p>In that same discussion, Maxi Ferreira and Avi Flombaum brought up the critique that LLMs will tend to choose the most common languages and frameworks when trying to solve a problem, even when there are better tools available. This is a variation of the observation that LLMs by default tend to produce a consensus solution. But the discussion highlighted for me that this represents a risk to skill acquisition and learning of up-and-coming developers too. It also made me wonder about the future of programming languages. Why develop new languages if there&#8217;s never going to be enough training data for LLMs to use them?</p> <p>Almost all of the speakers talked about the importance of up-front design when programming with AI. Harper Reed said that this sounds like a return to waterfall, except that the cycle is so fast. Clay Shirky <a href="https://pahlkadot.medium.com/dear-governor-elect-72e2f5e3bfdb" target="_blank" rel="noreferrer noopener">once observed</a> that waterfall development “amounts to a pledge by all parties not to learn anything while doing the actual work,” and that failure to learn while doing has hampered countless projects. But if AI codegen is waterfall with a fast learning cycle, that’s a very different model. So this is an important thread to pull on.</p> <p>Lili Jiang’s closing emphasis that evals are much more complex with LLMs really resonated for me, and was consistent with many of the speakers’ takes about how much further we have to go. Lili compared a data science project she had done at Quora, where they started with a carefully curated dataset (which made eval relatively easy), with trying to deal with self-driving algorithms at Waymo, where you don’t start out with “ground truth” and the right answer is highly context dependent. She asked, “How do you evaluate an LLM given such a high degree of freedom in terms of its output?” and pointed out that the code to do evals properly can be as large or larger than the code used to shape the actual functionality.</p> <p>This totally fits with my sense of why anyone imagining a programmer-free future is out of touch. AI makes some things that used to be hard trivially easy and some things that used to be easy much, much harder. Even if you had an LLM as judge doing the evals, there’s an awful lot to be figured out.</p> <p>I want to finish with Kent Beck’s thoughtful perspective on how different mindsets are needed at different stages in the evolution of a new market.</p> <iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/wh-kwZ6Kvdo?si=lik3tDo7jzh4XY6x" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> <p></p> <p>Finally, a big THANK YOU to everyone who gave their time to be part of our first AI Codecon event. Addy Osmani, you were the perfect cohost. You’re knowledgeable, a great interviewer, charming, and a lot of fun to work with. Gergely Orosz, Kent Beck, Camille Fournier, Avi Flombaum, Maxi Ferreira, Harper Reed, Jay Parikh, Birgitta Böckeler, Angie Jones, Craig McLuckie, Patty O’Callaghan, Chip Huyen, swyx Wang, Andrew Stellman, Iyanuoluwa Ajao, Nikola Balic, Brett Smith, Chelsea Troy, Lili Jiang—you all rocked. Thanks so much for sharing your expertise. Melissa Duffield, Julie Baron, Lisa LaRew, Keith Thompson, Yasmina Greco, Derek Hakim, Sasha Divitkina, and everyone else at O’Reilly who helped bring AI Codecon to life, thanks for all the work you put in to make the event a success. And thanks to the almost 9,000 attendees who gave your time, your attention, and your provocative questions in the chat.</p> <p>Subscribe to <a href="https://www.youtube.com/oreillymedia">our YouTube channel</a> to watch highlights from the event or <a href="https://www.oreilly.com/" target="_blank" rel="noreferrer noopener">become an O’Reilly member</a> to watch the entire conference before the next one September 9. We’d love to hear what landed for you—let us know in the comments.</p> ]]>
</content:encoded>
</item>
<item>
<title>Vibing at Home</title>
<link>https://www.oreilly.com/radar/vibing-at-home/</link>
<pubDate>Tue, 13 May 2025 10:08:01 +0000</pubDate>
<dc:creator>
<![CDATA[ Mike Loukides ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Signals ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16615</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/02/in-dis-canyon-15a-1400x950-1.jpg" medium="image" type="image/jpeg"/>
<description>
<![CDATA[ After a post by Andrej Karpathy went viral, “vibe coding” became the buzzword of the year—or at least the first quarter. It means programming exclusively with AI, without looking at or touching the code. If it doesn’t work, you have the AI try again, perhaps with a modified prompt that explains what went wrong. Simon [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>After a <a href="https://twitter.com/karpathy/status/1886192184808149383" target="_blank">post</a> by Andrej Karpathy went viral, “vibe coding” became the buzzword of the year—or at least the first quarter. It means programming exclusively with AI, without looking at or touching the code. If it doesn’t work, you have the AI try again, perhaps with a modified prompt that explains what went wrong. Simon Willison has an <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/" target="_blank">excellent blog post</a> about what vibe coding means, when it’s appropriate, and how to do it. While Simon is very positive about vibe coding, he’s <a href="https://simonwillison.net/2025/Mar/23/semantic-diffusion/" target="_blank">frustrated</a> that few of the people who are talking about it have read to the end of Karpathy’s tweet, where he says that vibe coding is most appropriate for weekend projects. Karpathy apparently agrees; he posted this <a href="https://x.com/karpathy/status/1903870973126045712" target="_blank">response</a>:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p>…In practice I rarely go full out vibe coding, and more often I still look at the code, I add complexity slowly and I try to learn over time how the pieces work, to ask clarifying questions etc.</p> </blockquote> <p>I’ve been experimenting with vibe coding over the past few months. I’ll start with a disclaimer: While I’ve been programming for a long time, I’m not (and have never been) a professional programmer. My programming consists of “weekend projects” and quick data analyses for O’Reilly. When vibe coding, I stayed away from tools like GitHub Copilot and Cursor, even though I was tempted—particularly by Claude Code, which may give us our best look at the future of programming. I wanted to keep the vibing experience pure, so I gave the model a prompt, copied the output, pasted it into a file, and ran it. I looked at it on occasion—Who wouldn’t?—but never edited it to fix bugs. Edits were limited to two situations: adding a comment saying which model generated the code (in retrospect, that should have been built into the prompt) and filling in dummy filenames and URLs that I used to keep private data away from publicly available models.</p> <p>Vibe coding works. Not all the time, and you may have to work hard to get the AI to deliver professional quality code. But with patience you’ll get working code with less effort than writing it yourself. Here are my observations:</p> <ul class="wp-block-list"> <li>You have to tell the model exactly what you want: what the inputs are, what the outputs are, and (often) how to get from the inputs to the outputs.&nbsp;</li> <li>If there’s more than one algorithm that might work, you need to tell the model which algorithm to use (if you care, and you may not). You can often get away with “Re-do the program with something that’s computationally efficient.”&nbsp;</li> <li>AI is very good at finding ways to slightly misinterpret what you said; you can feel like you’re talking to the witches in <em>Macbeth</em>.&nbsp;</li> <li>While it’s certainly possible to complain about the quality of AI-generated code, I found that the generated code was at least as good as what I would have written.&nbsp;</li> <li>AI isn’t bad at writing tests, but it’s poor at picking test cases.&nbsp;</li> <li>The AI included a lot of error checking and exception catching—frankly, enough to be annoying. But all those extra checks would be useful in software destined for production or that would be distributed to other users.&nbsp;</li> <li>Getting the AI to fix bugs was surprisingly easy. Pasting an error message into the chat was often enough; for more subtle errors (incorrect results rather than errors), “The result X was wrong for the input Y” was usually effective. Granted, this wasn’t a million-line enterprise project, where bugs might result from conflicts between modules that were written in different decades.</li> </ul> <p>So much for quick observations. Here’s some more detail.</p> <p>I complained about AI’s ability to generate good test cases. One of my favorite tasks when trying out a new model is asking an AI to write a program that checks whether numbers are prime. But how do you know whether the program works? I have a file that contains all the prime numbers under 100,000,000, so to vibe code some tests, I asked a model to write a test that selected some numbers from that file and determine whether they are prime. It chose the first five numbers (2, 3, 5, 7, 11) as test cases. Not much of a test. By the time I told it “Choose prime numbers at random from the file; and, to test non-prime numbers, choose two prime numbers and multiply them,” I had a much longer and more awkward prompt. I had similar results in other situations; if it wasn’t pushed, the model chose overly simple test cases.</p> <p>Algorithm choice can be an issue. My first attempt at vibe coding prime number tests yielded the familiar brute-force approach: Just try dividing. That’s nowhere near good enough. If I told the model I wanted to use the <a href="https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test" target="_blank" rel="noreferrer noopener">Miller-Rabin algorithm</a>, I got it, with only minor bugs. Using another model, I asked it to use an algorithm with good performance—and I got Miller-Rabin, so prompts don’t always have to be painfully explicit. When I tried asking for <a href="https://en.wikipedia.org/wiki/AKS_primality_test" target="_blank" rel="noreferrer noopener">AKS</a>—a more complicated test that is guaranteed to deliver correct results (Miller-Rabin is “probabilistic”; it can make mistakes)—the model told me that implementing AKS correctly was difficult, so it gave me Miller-Rabin instead. Enough said, I suppose. I had a similar experience asking for code to compute the <a href="https://en.wikipedia.org/wiki/Determinant" target="_blank" rel="noreferrer noopener">determinant</a> of a matrix. The first attempt gave me a simple recursive implementation that completed in factorial time—elegant but useless. If I asked explicitly for <a href="https://en.wikipedia.org/wiki/LU_decomposition" target="_blank" rel="noreferrer noopener">LU decomposition</a>, I got an acceptable result using Python NumPy libraries to do the work. (The LU approach is O(N**3).) I also tried asking the model not to use the libraries and to generate the code to do the decomposition; I couldn’t get this to work. Which wasn’t much fun, but in real life, libraries are your friend. Just make sure that any libraries an AI imports actually exist; don’t become a victim of <a href="https://devops.com/ai-generated-code-packages-can-lead-to-slopsquatting-threat/" target="_blank" rel="noreferrer noopener">slopsquatting</a>.</p> <p>It pays not to embed constants in your code—which, in this context, means “in your prompts.” When writing a program to work on a spreadsheet, I told the AI to use the third tab rather than specifying the tab by name. The program it generated worked just fine—it knew that pandas is zero-based, so there was a nice 2 in the code. But I was also curious about the Polars library, which I’ve never used. I didn’t want to throw my Gemini session off course, so I pasted the code into Claude and asked it to convert it to Polars. Claude rewrote the code directly—except that 2 remained 2, and Polars is 1-based, not zero-based, so I had some debugging to do. This may sound like a contrived example, but moving from one model to another or starting a new session to clear out old context is common. The moral of the story: We already know that it’s a good idea to keep constants out of your code and to write code that is easy for a human to understand. That goes double for your prompts. Prompt so that the AI generates code that will be easy for an AI—and for a human—to understand.</p> <p>Along similar lines: Never include credentials (usernames, passwords, keys) in your prompts. You don’t know where that’s going to end up. Read data like that from a configuration file. There are many more considerations about how to handle this kind of data securely, but keeping credentials out of your code is a good start. Google Drive provides a nice way to do this (and, of course, Gemini knows about it). Filenames and URLs for online data can also be sensitive. If you’re concerned (as I was when working with company data), you can say “Use a dummy URL; I’ll fill it in before running the program.”</p> <p>I tried two approaches to programming: starting small and working up, and starting with as complete a problem description as I could. Starting small is more typical of my own programming—and similar to the approach that Karpathy described. For example, if I’m working with a spreadsheet, I usually start by writing code to read the spreadsheet and report the number of rows. Then I add computational steps one at a time, with a test after each—maybe this is my personal version of “Agile.” Vibe coding like this allowed me to detect errors and get the AI to fix them quickly. Another approach is to describe the entire problem at once, in a single prompt that could be hundreds of words long. That also worked, though it was more error prone. It was too easy for me to issue a megaprompt, try the code, wonder why it didn’t work, and realize that the bug was my own, not the AI’s: I had forgotten to include something important. It was also more difficult to go back and tell the AI what it needed to fix; sometimes, it was easier to start a new session, but that also meant losing any context I’d built up. Both approaches can work; use whatever feels more comfortable to you.</p> <p>Almost everyone who has written about AI-assisted programming has said that it produces working code so quickly that they were able to do things that they normally wouldn’t have bothered to do—creating programs they wanted but didn&#8217;t really need, trying alternative approaches, working in new languages, and so on. “Yes” to all of this. For my spreadsheet analysis, I started (as I usually do) by downloading the spreadsheet from Google Drive—and normally, that’s as far as I would have gone. But after writing a program in 15 minutes that probably would have taken an hour, I said, “Why not have the program download the spreadsheet?” And then, “Why not have the program grab the data directly, without downloading the spreadsheet?” And then finally, “Accessing the data in place was slow. But a lot of the spreadsheets I work on are large and take time to download: What about downloading the spreadsheet only if a local copy doesn’t already exist?” Again, just another minute or so of vibing—and I learned a lot. Unfortunately, one thing I learned was that automating the download required the user to do more work than downloading the file manually. But at least now I know, and there are situations where automation would be a good choice. I also learned that the current models are good at adding features without breaking the older code; at least for shorter programs, you don’t have to worry much about AI rewriting code that’s already working.</p> <p>The online AI chat services<sup>1</sup> were, for the most part, fast enough to keep me in a “flow” where I could be thinking about what I was doing rather than waiting for output. Though as programs grew longer, I started to get impatient, even to the point of saying, “Don’t give me so much explanation, just give me the code.” I can certainly understand Steve Yegge’s <a href="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/genai_in_the_real_world_with_steve_yegge_1.mp3#t=1572">prediction</a> that the next step will be dashboards that let us keep several models busy simultaneously. I also tried running smaller models on my laptop,<sup>2</sup> focusing on Gemma 3 (4B), QwQ (32B), and DeepSeek R1 (32B). That was more of a “hurry up and wait” experience. It took several minutes to get from a prompt to usable code, even when I wasn’t using a “reasoning” model. A GPU would have helped. Nevertheless, working locally was a worthwhile experiment. The smaller models were slightly more error prone than the large models. They would definitely be useful in an environment where you have to worry about information leakage—for example, working with company financials or medical records. But expect to spend money on a high-end laptop or desktop (at least 64GB RAM and an NVIDIA GPU) and a lot of time drinking coffee while you wait.</p> <p>So, where does that leave us? Or, more appropriately, me? Vibe coding was fun, and it no doubt made me more efficient. But at what point does using AI become a crutch? I program infrequently enough that consistent vibe coding would cause my programming skills to degrade. Is that a problem? <a href="https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0174%3Atext%3DPhaedrus%3Apage%3D275#:~:text=Plato%20has%20written%20about%20writing%20and%20memory,beings%20but%20remain%20silent%20when%20asked%20questions." target="_blank" rel="noreferrer noopener">Plato</a> worried that literacy was a threat to memory—and he was very likely correct, at least in some respects. We no longer have wandering bards who have memorized all of literature. Do we care? When I started programming, I loved PDP-8 assembly. Now assembly language programmers are a small group of specialists; it’s largely irrelevant unless you’re writing device drivers. Looking back, I don’t think we’ve lost much. It’s always seemed like the fun in programming was about making a machine do what you wanted rather than solving language puzzles—though I’m sure many disagree.</p> <p>We still need programming skills. First, it was useful for me to see how my spreadsheet problem could be solved using Polars rather than pandas. (The Polars version felt faster, though I didn’t measure its performance.) It was also useful to see how various numerical algorithms were implemented—and understanding something about the algorithms proved to be important. And as much as we might like to say that programming is about solving problems, not learning programming languages, it’s very difficult to learn how to solve problems when you’re abstracted from the task of actually solving them. Second, we’ve all read that AI will liberate us from learning the dark corners of programming languages. But we all know that AI makes mistakes—fewer now than two or three years ago, but the mistakes are there. The frequency of errors will probably approach zero asymptotically but will never go to zero. And an AI isn’t likely to make simple mistakes like forgetting the parens on a Python print() statement or mismatching curly braces in Java. It’s liable to screw up precisely where we would: in the dark corners, because those dark corners don’t appear as often in the training data.</p> <p>We’re at a crossroads. AI-assisted programming is the future—but learning how to program is still important. Whether or not you go all the way to vibe coding, you will certainly be using some form of AI assistance. The tools are already good, and they will certainly get better. Just remember: Whatever writes the code, whoever writes the code, it’s your responsibility. If it’s a quick personal project, it can be sloppy—though you’re still the one who will suffer if your quick hack on your electronic locks keeps you out of your house. If you’re coding for work, you’re responsible for quality. You’re responsible for security. And it’s very easy to check in code that looks good only to find that fixing it becomes a drain on your whole group. Don’t let vibe coding be an excuse for laziness. Experiment with it, play with it, and learn to use it well. And continue to learn.</p> <hr class="wp-block-separator has-alpha-channel-opacity"/> <h3 class="wp-block-heading">Footnotes</h3> <ol class="wp-block-list"> <li>I worked mostly with Gemini and Claude; the results would be similar with ChatGPT.</li> <li> Macbook Pro (2019 Intel), 64 GB RAM. You don’t need a GPU but you do need a lot of RAM.</li> </ol> ]]>
</content:encoded>
<enclosure url="https://cdn.oreillystatic.com/radar/generative-ai-real-world-podcast/genai_in_the_real_world_with_steve_yegge_1.mp3" length="38412372" type="audio/mpeg"/>
</item>
<item>
<title>AI and Programming: The Beginning of a New Era</title>
<link>https://www.oreilly.com/radar/ai-and-programming-the-beginning-of-a-new-era/</link>
<pubDate>Thu, 08 May 2025 15:23:31 +0000</pubDate>
<dc:creator>
<![CDATA[ Tim O’Reilly ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Research ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16625</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2020/03/in-dis-canyon-14b-1400x950-1.jpg" medium="image" type="image/jpeg"/>
<description>
<![CDATA[ Our AI Codecon conference kicked off today with Coding with AI: The End of Software Development as We Know It. Here are my opening remarks introducing the series’ themes. You can reserve your seat for upcoming AI Codecon events here. Thanks so much for joining us today. We have over 20,000 people signed up for [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p><em>Our AI Codecon conference kicked off today with Coding with AI: The End of Software Development as We Know It. Here are my opening remarks introducing the series’ themes. You can reserve your seat for upcoming AI Codecon events </em><a href="https://www.oreilly.com/live/" target="_blank" rel="noreferrer noopener"><em>here</em></a><em>.</em></p> <p>Thanks so much for joining us today. We have over 20,000 people signed up for this event, both subscribers on the <a href="http://oreilly.com/">O&#8217;Reilly learning platform</a> and those who aren’t yet subscribers. I think you’re here because you all sense what I do: We&#8217;re witnessing not the end of programming but its remarkable expansion. This is the most exciting moment in software development that I&#8217;ve seen during my more than 40 years in this industry.</p> <p>I organized this event because I&#8217;ve grown increasingly frustrated with a persistent narrative: that AI will replace programmers. I&#8217;ve heard versions of this same prediction with every technological leap forward—and it&#8217;s always been wrong. Not just slightly wrong, but fundamentally misunderstanding how technology evolves.</p> <p>Programming, at its essence, is conversation with computers. It&#8217;s how we translate human intention into machine action. Throughout computing history, we&#8217;ve continuously built better translation layers between human thought and machine execution—from physical wiring to assembly language to high-level languages to the World Wide Web, which embedded calls to backend systems into a frontend made up of human-readable documents. LLMs are simply the next evolution in this conversation, making access to computer power more natural and accessible than ever before.</p> <p>And here&#8217;s what history consistently shows us: Whenever the barrier to communicating with computers lowers, we don&#8217;t end up with fewer programmers—we discover entirely new territories for computation to transform.</p> <p>There’s a kind of punctuated equilibrium, in which some breakthrough resets the industry, there’s a period of furious innovation followed by market consolidation, and frankly, a bit of stasis, until some new technology upsets the apple cart and sets off another period of reinvention.</p> <h2 class="wp-block-heading"><strong>The Historical Pattern of Expansion</strong></h2> <p>Consider how dramatically programming has evolved over the decades. It used to be really hard to tell computers what we wanted them to do. The earliest programmers had to physically connect circuits to execute different operations. Then came the von Neumann stored program architecture. That let programmers provide binary instructions through front panel switches. That was followed by assembly language, then compilers that took high-level, more-human-like descriptions and automatically translated them into the machine code that matched the architecture of the underlying system. With the World Wide Web, the interface to computers became human-readable <em>documents</em> that had some of the characteristics of a program. Links didn’t just summon new pages but ran other programs. Each step made the human-machine conversation more natural.</p> <p>With each evolution, skeptics predicted the obsolescence of &#8220;real programming.&#8221; Real programmers debugged with an oscilloscope. Yet the opposite occurred. The field expanded, creating new specialties and bringing more people into the conversation.</p> <p>Take the digital spreadsheet—a revolutionary tool that changed business forever. Dan Bricklin and Bob Frankston first prototyped VisiCalc in BASIC, the 1970s equivalent of today&#8217;s &#8220;vibe coding.&#8221; To create a viable product, they then rewrote it in assembly language for the 6502 microprocessor, the CPU for the Apple II. They had to do it this way to optimize performance and fit the program within the Apple II&#8217;s memory constraints. This pattern is instructive: Simplified tools enable rapid prototyping and experimentation, while deeper technical knowledge remains essential for production.</p> <p>Twenty years later, Tim Berners-Lee created the World Wide Web prototype on a NeXT machine—another leap forward in programming accessibility. So many of us learned to build our first web page simply by pulling down a menu, clicking “View Source,” and modifying the simple HTML code. Many of the people who created billion-dollar businesses on the early web began as amateur programmers. Many of them told me that they learned what they needed to know from an O’Reilly book.</p> <h2 class="wp-block-heading"><strong>AI-Assisted Programming Today: Democratization on Steroids</strong></h2> <p>That same pattern is repeating now—but at unprecedented scale and speed.</p> <p>Recently, a tech executive told me about his high-school-age daughter&#8217;s summer internship with a Stanford biomedical professor. Despite having no programming background—her interests were in biology and medicine—she was tasked with an ambitious challenge. The professor pointed out that pulse oximeters don’t work very well; the only way to get a good blood oxygen reading is with a blood draw. He said, “I have an idea that it might be possible to get a good reading out of the capillaries in the retina. Why don’t you look into that?” So she did. She fed ChatGPT lots of images of retinas, got it to isolate the capillaries, and then asked how it might detect oxygen saturation. That involved some coding. Pretty gnarly image recognition that normally would have taken a lot of programming experience to write. But by the end of the summer, she had a working program that was able to do the job.</p> <p>Now it’s easy to draw the conclusion from a story like this that this is the end of professional programming, that AI can do it all. For me, the lesson is the complete opposite. Pre-AI, investigating an idea like this would have meant taking it seriously enough to write a grant application, hire a researcher and a programmer, and give it a go. Now, it’s tossed off to a high school intern! What that shouts to me is that <strong>the cost of trying new things has gone down by orders of magnitude. And that means that the addressable surface area of programming has gone up by orders of magnitude. </strong>There’s so much more to do and explore.</p> <p>And do you think that that experiment is the end of this project? Is this prototype the finished product? Of course not. Turning it into something robust, reliable, and medically valid will require professional software engineers who understand systems design, testing methodologies, regulatory requirements, and deployment at scale.</p> <p><strong>Right now, we’re seeing a lot of people reengineering old ideas to do them better with AI. The next stage is going to be tackling entirely new problems, things that we couldn’t have—or wouldn’t have bothered to try—without AI.</strong></p> <h2 class="wp-block-heading"><strong>The New Spectrum: From Vibe Coding to AI Engineering</strong></h2> <p>What&#8217;s emerging is a new spectrum of software creation. At one end is &#8220;vibe coding&#8221;—rapid, intuitive programming assisted by AI. At the other end is systematic AI engineering—the disciplined integration of models into robust systems.</p> <p>This mirrors the evolution of the web. What began as simple static HTML pages evolved into complex, interconnected systems with frameworks, APIs, and cloud infrastructure—what I called in 2005 &#8220;software above the level of a single device.&#8221; The web didn&#8217;t eliminate programming jobs; it created entirely new categories of development work. Frontend engineering, backend engineering, DevOps, information security. More JavaScript frameworks than anyone can keep track of!</p> <p>We&#8217;re seeing that same pattern with LLMs and agents. The raw model is just the beginning—like HTML was to the web. The real magic happens in how these models are integrated, refined, and deployed as components in larger systems.</p> <h2 class="wp-block-heading"><strong>The New Hybrid Computing Paradigm</strong></h2> <p>A tool like ChatGPT, Perplexity, or Cursor highlights just how much more there is to an AI application than the model. <strong>The naked model is dressed in fashions dreamed up by entrepreneurs, shaped by product managers, and pieced together by AI engineers.</strong> Any AI app (including just a chatbot) is actually a hybrid of AI and traditional software engineering.</p> <p>In a recent conversation in a private chat group, Eran Sandler used a car metaphor: &#8220;The model is the engine, but you need a whole lot around it to make it a sports car—context management, codified workflows, and more. Those are the &#8216;real uses&#8217; of AI models.&#8221;</p> <p>This reminded me of Phillip Carter&#8217;s insight that we&#8217;re now programming with two fundamentally different types of computers: one that can write poetry but struggles with basic arithmetic, another that calculates flawlessly but lacks creativity. The art of modern development is orchestrating these systems to complement each other.</p> <p>Sam Schillace added another dimension: &#8220;There&#8217;s now a tension between reliable and flexible—code is reliable but rigid, inference is flexible but unreliable.&#8221; He described how the new job of the programmer is to craft carefully designed &#8220;metacognitive recipes&#8221;—code that manages and directs AI inference. Doing this well can transform a task from 5%–10% reliable to nearly 100% in specific domains.</p> <p>These conversations reveal the future landscape. We&#8217;re not at the end of programming—we&#8217;re at the beginning of its most profound reinvention yet.</p> <h2 class="wp-block-heading"><strong>A Renaissance of Innovation</strong></h2> <p>It&#8217;s an extraordinary time to be in software development. After years of incremental advances that made the field feel somewhat predictable, we&#8217;re entering a period of radical innovation. The fundamental building blocks of how we create software are changing.</p> <p>This isn&#8217;t just about using AI tools to write code faster—though that&#8217;s valuable. It&#8217;s about reimagining what software can do, who can create it, and how we approach problems that previously seemed intractable.</p> <p>This conference will explore three critical dimensions of this new landscape:</p> <ul class="wp-block-list"> <li>How to effectively collaborate with AI to enhance your current development workflow</li> <li>The emerging patterns and antipatterns of building reliable, production-grade AI systems</li> <li>The expanding opportunity landscape as previously infeasible projects become possible</li> </ul> <p>The programming world was frankly getting a bit predictable for a while. The fun is back—along with unprecedented opportunity. Throughout this event, I hope you&#8217;ll not just absorb information but actively consider: What problem that seemed impossible yesterday might you now be able to solve?</p> <p>Let&#8217;s embrace this moment not with fear but with the excitement of explorers discovering new territory.</p> ]]>
</content:encoded>
</item>
<item>
<title>MCP: What It Is and Why It Matters—Part 1</title>
<link>https://www.oreilly.com/radar/mcp-what-it-is-and-why-it-matters-part-1/</link>
<pubDate>Thu, 08 May 2025 09:57:56 +0000</pubDate>
<dc:creator>
<![CDATA[ Addy Osmani ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Deep Dive ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16609</guid>
<description>
<![CDATA[ This is the first of four parts in this series. 1. ELI5: Understanding MCP Imagine you have a single universal plug that fits all your devices—that&#8217;s essentially what the Model Context Protocol (MCP) is for AI. MCP is an open standard (think “USB-C for AI integrations”) that allows AI models to connect to many different [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p class="has-black-color has-text-color has-background has-link-color wp-elements-50de1d342e4cb762e386c31d6779afcf" style="background:linear-gradient(135deg,rgb(238,238,238) 100%,rgb(169,184,195) 100%)"><em>This is the first of four parts in this series.</em></p> <h2 class="wp-block-heading"><strong>1. ELI5: Understanding MCP</strong></h2> <p>Imagine you have a single universal <strong>plug</strong> that fits all your devices—that&#8217;s essentially what the <strong>Model Context Protocol (MCP)</strong> is for AI. MCP is an <a rel="noreferrer noopener" href="https://www.anthropic.com/news/model-context-protocol" target="_blank"><strong>open standard</strong></a> (think “<strong>USB-C for AI integrations</strong>”) that allows AI models to connect to many different apps and data sources in a consistent way. In simple terms, MCP lets an AI assistant talk to various software tools using a common language, instead of each tool requiring a different adapter or custom code.</p> <figure class="wp-block-image"><img decoding="async" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZtcLZfy8ZhLcG_Tjum6Nomnb9f6Fc7lb9jaL9XasG7GjkjuoAohG0ShKbv-XmwyCuhMevoqbzVfUqZxNwFvMFunfaC10HQKdBMlNZl13EtpQgp080j59zSXdbcbIjS3GeAO3CEw?key=Dp_trz-zeIyFtl2opXn4Ynl8" alt="This image has an empty alt attribute; its file name is AD_4nXdZtcLZfy8ZhLcG_Tjum6Nomnb9f6Fc7lb9jaL9XasG7GjkjuoAohG0ShKbv-XmwyCuhMevoqbzVfUqZxNwFvMFunfaC10HQKdBMlNZl13EtpQgp080j59zSXdbcbIjS3GeAO3CEw"/></figure> <p>So, what does this mean in practice? If you’re using an AI coding assistant like Cursor or Windsurf, MCP is the <strong>shared protocol</strong> that lets that assistant use external tools on your behalf. For example, with MCP an AI model could <strong>fetch information from a database, edit a design in Figma, or control a music app</strong>—all by sending natural-language instructions through a standardized interface. You (or the AI) no longer need to manually switch contexts or learn each tool’s API; the <strong>MCP “translator” bridges the gap between human language and software commands</strong>.</p> <p>In a nutshell, MCP is like giving your AI assistant a <strong>universal remote control</strong> to operate all your digital devices and services. Instead of being stuck in its own world, your AI can now reach out and press the buttons of other applications safely and intelligently. This common protocol means <strong>one AI can integrate with thousands of tools</strong> as long as those tools have an MCP interface—eliminating the need for custom integrations for each new app. The result: Your AI helper becomes far more capable, able to not just chat about things but <strong>take actions in the real software you use</strong>.</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><img src="https://s.w.org/images/core/emoji/15.1.0/72x72/1f9e9.png" alt="🧩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Built an MCP that lets Claude talk directly to Blender. It helps you create beautiful 3D scenes using just prompts!</p> <p>Here’s a demo of me creating a “low-poly dragon guarding treasure” scene in just a few sentences<img src="https://s.w.org/images/core/emoji/15.1.0/72x72/1f447.png" alt="👇" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p> <p><em>Video: </em><a href="https://x.com/sidahuj/status/1899460492999184534" target="_blank" rel="noreferrer noopener"><em>Siddharth Ahuja</em></a></p> </blockquote> <h2 class="wp-block-heading"><strong>2. Historical Context: From Text Prediction to Tool-Augmented Agents</strong></h2> <p>To appreciate MCP, it helps to recall how AI assistants evolved. Early large language models (LLMs) were essentially clever <strong>text predictors</strong>: Given some input, they’d generate a continuation based on patterns in training data. They were powerful for answering questions or writing text but <strong>functionally isolated</strong>—they had <strong>no built-in way to use external tools or real-time data</strong>. If you asked a 2020-era model to check your calendar or fetch a file, it couldn’t; it only knew how to produce text.</p> <p><strong>2023</strong> was a turning point. AI systems like ChatGPT began to integrate “<strong>tools</strong>” and plug-ins. OpenAI introduced function calling and plug-ins, allowing models to execute code, use web browsing, or call APIs. Other frameworks (LangChain, AutoGPT, etc.) emerged, enabling multistep “agent” behaviors. These approaches let an LLM act more like an <strong>agent</strong> that can plan actions: e.g., search the web, run some code, then answer. However, in these early stages each integration was <strong>one-off and ad hoc</strong>. Developers had to wire up each tool separately, often using different methods: One tool might require the AI to output JSON; another needed a custom Python wrapper; another a special prompt format. There was <strong>no standard way</strong> for an AI to know what tools are available or how to invoke them—it was all hard-coded.</p> <p>By <strong>late 2023</strong>, the community realized that to fully unlock AI agents, we needed to move beyond treating LLMs as solitary oracles. This gave rise to the idea of <strong>tool-augmented agents</strong>—AI systems that can <strong>observe, plan, and act</strong> <strong>on</strong> the world via software tools. Developer-focused AI assistants (Cursor, Cline, Windsurf, etc.) began embedding these agents into IDEs and workflows, letting the AI read code, call compilers, run tests, etc., in addition to chatting. Each tool integration was immensely powerful but <strong>painfully fragmented</strong>: One agent might control a web browser by generating a Playwright script, while another might control Git by executing shell commands. There was no unified “language” for these interactions, which made it <strong>hard to add new tools or switch AI models</strong>.</p> <p>This is the backdrop against which Anthropic (the creators of the Claude AI assistant) introduced MCP in <strong>late 2024</strong>. They recognized that as LLMs became more capable, the <strong>bottleneck was no longer the model’s intelligence but its connectivity</strong>. Every new data source or app required bespoke glue code, slowing down innovation. MCP emerged from the need to <strong>standardize the interface</strong> between AI and the wide world of software—much like establishing a common protocol (HTTP) enabled the web’s explosion. It represents the natural next step in LLM evolution: from pure text prediction to agents with tools (each one custom) to <strong>agents with a universal tool interface</strong>.</p> <h2 class="wp-block-heading"><strong>3. The Problem MCP Solves</strong></h2> <p>Without MCP, integrating an AI assistant with external tools is a bit like having a bunch of appliances each with a different plug and no universal outlet. Developers were dealing with <strong>fragmented integrations</strong> everywhere. For example, your AI IDE might use one method to get code from GitHub, another to fetch data from a database, and yet another to automate a design tool—each integration needing a custom adapter. Not only is this labor-intensive; it’s brittle and doesn’t scale. <a rel="noreferrer noopener" href="https://www.anthropic.com/news/model-context-protocol" target="_blank">As Anthropic put it</a>:</p> <p><em>Even the most sophisticated models are constrained by their isolation from data</em>—<em>trapped behind information silos.…Every new data source requires its own custom implementation, making truly connected systems difficult to scale.</em></p> <p><strong>MCP addresses this fragmentation</strong> head-on by offering <strong>one common protocol</strong> for all these interactions. Instead of writing separate code for each tool, a developer can implement the MCP specification and instantly make their application accessible to any AI that speaks MCP. This <strong>dramatically simplifies the integration matrix</strong>: AI platforms need to support only MCP (not dozens of APIs), and tool developers can expose functionality once (via an MCP server) rather than partnering with every AI vendor separately.</p> <p>Another big challenge was <strong>tool-to-tool “language mismatch.”</strong> Each software or service has its own API, data format, and vocabulary. An AI agent trying to use them had to know all these nuances. For instance, telling an AI to fetch a Salesforce report versus querying a SQL database versus editing a Photoshop file are completely different procedures in a pre-MCP world. This mismatch meant the AI’s <strong>“intent” had to be translated into every tool’s unique dialect</strong>—often by fragile prompt engineering or custom code. MCP solves this by imposing a structured, self-describing interface: Tools can <strong>declare their capabilities in a standardized way</strong>, and the AI can invoke those capabilities through natural-language intents that the MCP server parses. In effect, MCP teaches all tools a bit of the <strong>same language</strong>, so the AI doesn’t need a thousand phrasebooks.</p> <p>The result is a much more <strong>robust and scalable architecture</strong>. Instead of building N×M integrations (N tools times M AI models), we have <strong>one protocol to rule them all</strong>. As Anthropic’s announcement described, MCP “replaces fragmented integrations with a single protocol,” yielding a <strong>simpler, more reliable way</strong> to give AI access to the data and actions it needs. This uniformity also paves the way for <strong>maintaining context across tools</strong>—an AI can carry knowledge from one MCP-enabled tool to another because the interactions share a common framing. In short, MCP tackles the integration nightmare by introducing a common connective tissue, enabling AI agents to plug into new tools <strong>as easily as a laptop accepts a USB device</strong>.</p> ]]>
</content:encoded>
</item>
<item>
<title>Radar Trends to Watch: May 2025</title>
<link>https://www.oreilly.com/radar/radar-trends-to-watch-may-2025/</link>
<pubDate>Tue, 06 May 2025 20:26:55 +0000</pubDate>
<dc:creator>
<![CDATA[ Mike Loukides ]]>
</dc:creator>
<category>
<![CDATA[ Radar Trends ]]>
</category>
<category>
<![CDATA[ Signals ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.com/radar/?p=16585</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2023/06/radar-1400x950-8.png" medium="image" type="image/png"/>
<custom:subtitle>
<![CDATA[ Developments in AI, Infrastructure, Web, and More ]]>
</custom:subtitle>
<description>
<![CDATA[ Anthropic’s Model Context Protocol (MCP) has received a lot of attention for standardizing the way models communicate with tools, making it much easier to build intelligent agents. Google’s Agent2Agent (A2A) now adds features that were left out of the original MCP specification: security, agent cards for describing agent capabilities, and more. Is A2A competitive or [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>Anthropic’s Model Context Protocol (MCP) has received a lot of attention for standardizing the way models communicate with tools, making it much easier to build intelligent agents. Google’s Agent2Agent (A2A) now adds features that were left out of the original MCP specification: security, agent cards for describing agent capabilities, and more. Is A2A competitive or complementary? Is it another layer in a developing protocol stack for agentic applications? Similarly, Claude Code has been the flagship for agentic coding, the next step beyond cut-and-paste and comment completion (GitHub) models. Now, with OpenAI’s terminal-based Codex and Google’s Firebase Studio IDE, it has competition. The upside for Anthropic? These tools implicitly acknowledge that Anthropic is the AI vendor to beat.</p> <h2 class="wp-block-heading">Artificial Intelligence</h2> <ul class="wp-block-list"> <li>OpenAI’s latest video generation model (gpt-image-1) is <a aria-label=" (opens in a new tab)" href="https://openai.com/index/image-generation-api/" target="_blank" rel="noreferrer noopener">now available via the company’s API</a>.&nbsp;</li> <li>The European Space Agency and IBM have created <a aria-label=" (opens in a new tab)" href="https://thenextweb.com/news/esa-ibm-launch-ai-model-with-intuitive-understanding-of-earth" target="_blank" rel="noreferrer noopener">TerraMind</a>, a generative AI model of the Earth. Among other things, the model has been trained for climate forecasting. It’s available on <a aria-label=" (opens in a new tab)" href="https://huggingface.co/ibm-esa-geospatial/TerraMind-1.0-base" target="_blank" rel="noreferrer noopener">Hugging Face</a>.&nbsp;</li> <li><a aria-label=" (opens in a new tab)" href="https://www.biographic.com/ping-youve-got-whale/" target="_blank" rel="noreferrer noopener">WhaleSpotter</a> is an AI-enabled thermal camera that ships can use to spot whales in time to change course and avoid collisions. The system detects the heat from a whale’s spout.</li> <li>Google’s latest reasoning model, <a aria-label=" (opens in a new tab)" href="https://blog.google/products/gemini/gemini-2-5-flash-preview/" target="_blank" rel="noreferrer noopener">Gemini 2.5 Flash</a>, is now available in preview. Flash is a “hybrid reasoning model” that allows users to specify a “thinking budget” so they can control how much money (time, tokens) is spent on reasoning.&nbsp;</li> <li><a aria-label=" (opens in a new tab)" href="https://github.com/pydantic/pydantic-ai/tree/main/mcp-run-python" target="_blank" rel="noreferrer noopener">MCP Run Python</a> is an MCP server from Pydantic for running LLM-generated Python code in a sandbox. Simon Willison has a couple of <a aria-label=" (opens in a new tab)" href="https://simonwillison.net/2025/Apr/18/mcp-run-python/#atom-everything" target="_blank" rel="noreferrer noopener">fascinating demos</a>.&nbsp;</li> <li>OpenAI has <a aria-label=" (opens in a new tab)" href="https://openai.com/index/introducing-o3-and-o4-mini/" target="_blank" rel="noreferrer noopener">launched</a> its o3 and o4-mini models. o3 is its most advanced reasoning model, and o4-mini is a smaller reasoning model designed to be faster and more cost-efficient. These new models replace o1 and o3-mini.</li> <li>A model for maritime navigation has demonstrated that <a aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2025-04-ai-ship-decreases-human-error.html" target="_blank" rel="noreferrer noopener">explaining the reason for navigational decisions increases trust and reduces human error</a>.&nbsp;</li> <li>OpenAI has <a aria-label=" (opens in a new tab)" href="https://openai.com/index/gpt-4-1/" target="_blank" rel="noreferrer noopener">released</a> GPT-4.1, including mini and nano versions. OpenAI claims that GPT-4.1 improves significantly on code generation and instruction following. All the models have a 1M token input window. The 4.1 series models are currently only available via the API. GPT-4 is slated to be <a aria-label=" (opens in a new tab)" href="https://techcrunch.com/2025/04/11/openai-is-winding-down-its-gpt-4-ai-model-in-chatgpt/" target="_blank" rel="noreferrer noopener">retired</a>, as is GPT-4.5 preview.&nbsp;</li> <li>A new paper from DeepMind describes some <a aria-label=" (opens in a new tab)" href="https://arstechnica.com/information-technology/2025/04/researchers-claim-breakthrough-in-fight-against-ais-frustrating-security-hole/" target="_blank" rel="noreferrer noopener">strategies</a> for <a aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2503.18813" target="_blank" rel="noreferrer noopener">defending against prompt injection</a> attacks. As Simon Willison <a aria-label=" (opens in a new tab)" href="https://simonwillison.net/2025/Apr/11/camel/#atom-everything" target="_blank" rel="noreferrer noopener">writes</a>, prompt injection has been around for two and a half years; this may be the first significant progress in defeating it.</li> <li>ChatGPT can now <a aria-label=" (opens in a new tab)" href="https://arstechnica.com/ai/2025/04/chatgpt-can-now-remember-and-reference-all-your-previous-chats/" target="_blank" rel="noreferrer noopener">reference your entire chat history</a>. This is a significant extension of its older Memory feature, which could only remember a few pieces of information.&nbsp;</li> <li>MCP may be the basis for the next generation of AI-driven technology, but <a aria-label=" (opens in a new tab)" href="https://elenacross7.medium.com/%EF%B8%8F-the-s-in-mcp-stands-for-security-91407b33ed6b" target="_blank" rel="noreferrer noopener">it’s important to remember security</a>. Protocol vulnerabilities are as dangerous as SQL injection—and MCP has many of them. (No doubt A2A does too; it goes with the territory.)</li> <li>Anthropic has announced a new <a aria-label=" (opens in a new tab)" href="https://www.anthropic.com/news/max-plan" target="_blank" rel="noreferrer noopener">Max Plan</a> for Claude users to mitigate complaints that users are bumping into their usage limits too often. Max is $100 or $200 a month, for 5x or 20x more usage than Pro. It’s not cheap, but bumping into limits is frustrating.</li> <li>For those of us who like keeping our AI close to home, there’s now <a aria-label=" (opens in a new tab)" href="https://www.together.ai/blog/deepcoder" target="_blank" rel="noreferrer noopener">DeepCoder</a>, a 14B model that specializes in coding and that claims performance similar to OpenAI’s o3-mini. Dataset, code, training logs, and system optimizations are all open.</li> <li>Two <a aria-label=" (opens in a new tab)" href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html" target="_blank" rel="noreferrer noopener">important</a> <a aria-label=" (opens in a new tab)" href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html" target="_blank" rel="noreferrer noopener">papers</a> from Anthropic give some clues about how agents think. And an <a aria-label=" (opens in a new tab)" href="https://www.noemamag.com/ai-is-evolving-and-changing-our-understanding-of-intelligence/" target="_blank" rel="noreferrer noopener">article</a> by Google’s Blaise Agüera y Arcas and James Manyika challenges our notions of how we think.</li> <li>Google has <a aria-label=" (opens in a new tab)" href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/" target="_blank" rel="noreferrer noopener">announced</a> its <a aria-label=" (opens in a new tab)" href="https://github.com/google/A2A" target="_blank" rel="noreferrer noopener">Agent2Agent</a> protocol (<a aria-label=" (opens in a new tab)" href="https://thenewstack.io/googles-agent2agent-protocol-helps-ai-agents-talk-to-each-other/" target="_blank" rel="noreferrer noopener">A2A</a>), to facilitate communications between intelligent agents. It provides communications between agents, agent discovery, and asynchronous task management. The company stresses that A2A is complementary to MCP.&nbsp;</li> <li>The Model Context Protocol is taking the AI world by storm. There are several projects listing MCP servers, including <a aria-label=" (opens in a new tab)" href="http://mcpservers.org" target="_blank" rel="noreferrer noopener">mcpservers.org</a>, the <a aria-label=" (opens in a new tab)" href="https://github.com/punkpeye/awesome-mcp-servers" target="_blank" rel="noreferrer noopener">awesome-mcp-servers</a> GitHub repo, <a aria-label=" (opens in a new tab)" href="https://glama.ai/mcp/servers" target="_blank" rel="noreferrer noopener">Glama’s list</a>, and Cline’s <a aria-label=" (opens in a new tab)" href="https://github.com/cline/mcp-marketplace" target="_blank" rel="noreferrer noopener">MCP Marketplace</a> (accessible through its plug-in).&nbsp;</li> <li>OpenAI is rolling out <a aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/artificial-intelligence/openai-tests-watermarking-for-chatgpt-4o-image-generation-model/" target="_blank" rel="noreferrer noopener">watermarks</a> for its image generation model, possibly in response to reactions to its “Studio Ghibli” filter. Users with a paid account can apparently save images without watermarks.&nbsp;</li> <li>Meta has <a aria-label=" (opens in a new tab)" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noreferrer noopener">released</a> the Llama 4 “herd” of open models. They’re all mixture-of-experts models with large context windows. Scout and Maverick both have 17B active parameters, with 16 and 128 “experts,” respectively; they’re available on <a aria-label=" (opens in a new tab)" href="https://www.llama.com/llama-downloads/" target="_blank" rel="noreferrer noopener">llama.com</a> and <a aria-label=" (opens in a new tab)" href="https://huggingface.co/meta-llama" target="_blank" rel="noreferrer noopener">Hugging Face</a>. Behemoth is a 288B active parameter (2T total) “teacher” model used to train other models.&nbsp;</li> <li>OpenAI is actually <a aria-label=" (opens in a new tab)" href="https://openai.com/open-model-feedback/" target="_blank" rel="noreferrer noopener">planning to release an open model</a>? Surprise, surprise. Needless to say, it hasn’t been released yet. But they want feedback already.</li> <li>Gemini 2.5 is now <a aria-label=" (opens in a new tab)" href="https://arstechnica.com/gadgets/2025/03/googles-new-experimental-gemini-2-5-model-rolls-out-to-free-users/" target="_blank" rel="noreferrer noopener">available</a> to free users; select Gemini 2.5 Pro (Experimental) in the Gemini app. Some of its capabilities are restricted (for example, free users can’t upload documents).&nbsp;</li> <li>Can an AI be a <a aria-label=" (opens in a new tab)" href="https://www.schneier.com/blog/archives/2025/03/ais-as-trusted-third-parties.html" target="_blank" rel="noreferrer noopener">trusted third party</a>? Can it make a judgment based on information from two sources without revealing the information on which the judgment was based? The answer may be “<a aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/2501.08970" target="_blank" rel="noreferrer noopener">yes</a>.” It helps that models can be deleted.</li> <li>Google’s open <a aria-label=" (opens in a new tab)" href="https://blog.google/technology/developers/gemma-3/" target="_blank" rel="noreferrer noopener">Gemma 3</a> models have taken several steps forward. They now support <a aria-label=" (opens in a new tab)" href="https://ai.google.dev/gemma/docs/capabilities/function-calling" target="_blank" rel="noreferrer noopener">function calling</a> and larger (128K) context windows. <a aria-label=" (opens in a new tab)" href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/" target="_blank" rel="noreferrer noopener">Quantization-aware training</a> optimizes their performance to make the models accessible for less-powerful hardware: a single GPU or even a GPU-less laptop.</li> </ul> <h2 class="wp-block-heading">Programming</h2> <ul class="wp-block-list"> <li>We do code reviews. Should we also do <a aria-label=" (opens in a new tab)" href="https://thenewstack.io/recce-believes-data-reviews-will-become-as-important-as-code-reviews/" target="_blank" rel="noreferrer noopener">data reviews</a>? As we become more dependent on AI and massive data pipelines, we need to know that our data is trustworthy.</li> <li>When using Claude Code, the <a aria-label=" (opens in a new tab)" href="https://www.anthropic.com/engineering/claude-code-best-practices" target="_blank" rel="noreferrer noopener">thinking budget is evidently controlled</a> by using the words “think,” “think hard,” “think harder,” and “ultrathink” in prompts.</li> <li><a aria-label=" (opens in a new tab)" href="https://thenewstack.io/kelsey-hightower-on-nix-vs-docker-is-there-a-different-way/" target="_blank" rel="noreferrer noopener">Kelsey Hightower sees</a> the <a aria-label=" (opens in a new tab)" href="https://nixos.org/" target="_blank" rel="noreferrer noopener">Nix project</a> as a possible complement to Docker. Using Nix inside of Docker files leads to more efficient and reproducible builds.</li> <li>OpenAI has also released <a aria-label=" (opens in a new tab)" href="https://github.com/openai/codex" target="_blank" rel="noreferrer noopener">Codex</a>, a coding agent that runs in the terminal. It appears to be similar to Claude Code, but it has an open source license.&nbsp;</li> <li>The <a aria-label=" (opens in a new tab)" href="https://github.com/kro-run/kro" target="_blank" rel="noreferrer noopener">kro project</a> (Kubernetes Resource Orchestrator) allows developers to build groups of Kubernetes resources that can be used to simplify Kubernetes cluster configurations in a vendor-independent way. </li> <li>Python now has a <a aria-label=" (opens in a new tab)" href="https://pypi.org/project/tariff/" target="_blank" rel="noreferrer noopener">tariff</a> package to tax imports! 50% on NumPy, 200% on pandas. As in the real world, you only tax yourself.</li> <li>Google’s <a aria-label=" (opens in a new tab)" href="https://firebase.studio/" target="_blank" rel="noreferrer noopener">Firebase Studio</a> is a generative AI-native IDE for building full stack web applications. It’s getting good reviews online. In addition to integration with Git and GitHub, it’s integrated into Google Cloud, so it can deploy applications automatically.</li> <li>OpenAI <a aria-label=" (opens in a new tab)" href="https://help.openai.com/en/articles/10910291-api-organization-verification" target="_blank" rel="noreferrer noopener">will require organization verification</a> for developers to gain API access to future models. Despite the name, this status applies to individual developers and will require a valid government-issued ID; IDs from over 200 countries are acceptable.</li> <li>Amazon’s Alexa has lost its shine, but the new Alexa+ is <a aria-label=" (opens in a new tab)" href="https://www.allthingsdistributed.com/2025/04/alexa-plus-gets-us-a-step-closer-to-ambient-interfaces.html" target="_blank" rel="noreferrer noopener">based on generative AI</a>. The company is looking for developers to <a aria-label=" (opens in a new tab)" href="https://developer.amazon.com/en-US/blogs/alexa/alexa-skills-kit/2025/02/new-alexa-announce-blog" target="_blank" rel="noreferrer noopener">test its AI-native SDKs</a>.</li> <li>Although Rust code is still a small part of the Linux kernel, its presence is <a aria-label=" (opens in a new tab)" href="https://thenewstack.io/rust-linux-and-cloud-native-computing/" target="_blank" rel="noreferrer noopener">growing</a>—and Rust’s memory safety is paying off.&nbsp;</li> <li>NVIDIA is <a aria-label=" (opens in a new tab)" href="https://thenewstack.io/nvidia-finally-adds-native-python-support-to-cuda/" target="_blank" rel="noreferrer noopener">adding native support for Python to CUDA</a>, its toolkit for programming GPUs.</li> <li>NVIDIA has also <a aria-label=" (opens in a new tab)" href="https://thenewstack.io/nvidia-making-radical-changes-to-cuda-after-nearly-20-years/" target="_blank" rel="noreferrer noopener">announced</a> that a future version of CUDA will allow developers to treat large clusters of GPUs as a single virtual GPU. There’s no estimate for when these new features will be released.</li> <li>Microsoft has <a aria-label=" (opens in a new tab)" href="https://microsoft.github.io/debug-gym/" target="_blank" rel="noreferrer noopener">published</a> a <a aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2503.21557" target="_blank" rel="noreferrer noopener">paper</a> about giving a code-generating LLM access to a Python debugger. Agentic vibe debugging, here we come!</li> <li><a aria-label=" (opens in a new tab)" href="https://thenewstack.io/endor-webassembly-based-server-in-the-browser/" target="_blank" rel="noreferrer noopener">Run a server in the browser</a>? With Wasm, why not? It’s not a good production environment, but it could be ideal for development and debugging.&nbsp;</li> <li>Rust finally has a <a aria-label=" (opens in a new tab)" href="https://rustfoundation.org/media/ferrous-systems-donates-ferrocene-language-specification-to-rust-project/" target="_blank" rel="noreferrer noopener">formal language specification</a>! The spec was developed and donated to the Rust Foundation by Ferrous Systems, a company that develops Rust compilers. I’m shocked that one didn’t already exist—but apparently one didn’t.</li> </ul> <h2 class="wp-block-heading">Security</h2> <ul class="wp-block-list"> <li><a aria-label=" (opens in a new tab)" href="https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/" target="_blank" rel="noreferrer noopener">Policy Puppetry</a> is a new prompt injection attack technique that works against all major LLMs. The attack works by writing the malicious prompt in a form that can be interpreted as a policy file that the LLM would be required to obey.</li> <li><a aria-label=" (opens in a new tab)" href="https://arstechnica.com/gadgets/2025/04/in-depth-with-windows-11-recall-and-what-microsoft-has-and-hasnt-fixed/" target="_blank" rel="noreferrer noopener">Windows Recall is back</a>. It’s in the preview channel. Many of the problems appear to have been fixed. It’s not on by default, it can be uninstalled, and it can be used without a network connection. But it’s still creepy, and Microsoft’s reputation is a problem that remains.</li> <li>Mitre’s CVE program (Common Vulnerabilities and Exposures) was almost defunded. Funding <a aria-label=" (opens in a new tab)" href="https://www.csoonline.com/article/3963190/cve-program-faces-swift-end-after-dhs-fails-to-renew-contract-leaving-security-flaw-tracking-in-limbo.html" target="_blank" rel="noreferrer noopener">expired</a> on <a aria-label=" (opens in a new tab)" href="https://www.nextgov.com/cybersecurity/2025/04/mitre-backed-cyber-vulnerability-program-lose-funding-wednesday/404585/" target="_blank" rel="noreferrer noopener">April 15</a> and was only <a aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/cisa-extends-funding-to-ensure-no-lapse-in-critical-cve-services/" target="_blank" rel="noreferrer noopener">extended</a> for 11 months on April 17. CVE has been <a aria-label=" (opens in a new tab)" href="https://www.theregister.com/2025/04/16/homeland_security_funding_for_cve/" target="_blank" rel="noreferrer noopener">essential</a> in disseminating information about security weaknesses in computer systems.&nbsp;</li> <li>Google has <a aria-label=" (opens in a new tab)" href="https://workspace.google.com/blog/identity-and-security/gmail-easy-end-to-end-encryption-all-businesses" target="_blank" rel="noreferrer noopener">announced</a> end-to-end encryption (e2e) for Gmail. While this reduces the burden of implementing e2e encryption for IT departments, it’s <a aria-label=" (opens in a new tab)" href="https://arstechnica.com/security/2025/04/are-new-google-e2ee-emails-really-end-to-end-encrypted-kinda-but-not-really/" target="_blank" rel="noreferrer noopener">debatable</a> whether this is truly e2e. Recipients who don’t use Gmail can use a special subset of Gmail to read encrypted mail.&nbsp;</li> <li><a aria-label=" (opens in a new tab)" href="https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/" target="_blank" rel="noreferrer noopener">OpenPubkey SSH</a> simplifies using SSH with single sign-on. It adds SSH public keys to the ID tokens used by <a aria-label=" (opens in a new tab)" href="https://openid.net/developers/how-connect-works/" target="_blank" rel="noreferrer noopener">OpenID Connect</a>. Short-lived SSH keypairs are created automatically when users sign in, and don’t need to be managed by users.</li> </ul> <h2 class="wp-block-heading">Infrastructure</h2> <ul class="wp-block-list"> <li>Microsoft is working on a tool that automates <a aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/microsoft/microsoft-tests-new-quick-machine-recovery-tool-to-fix-boot-crashes/" target="_blank" rel="noreferrer noopener">fixing Windows 11 boot crashes</a>. Boot crashes are typically caused by configuration errors or installing a bad device. A tool like this might have helped users to recover after the <a aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/crowdstrike-update-crashes-windows-systems-causes-outages-worldwide/" target="_blank" rel="noreferrer noopener">bad CrowdStrike update</a> last year. </li> </ul> <h2 class="wp-block-heading">Web</h2> <ul class="wp-block-list"> <li>Could OpenAI be the new Twitter? The company’s apparently in the early stages of <a aria-label=" (opens in a new tab)" href="https://www.theverge.com/openai/648130/openai-social-network-x-competitor" target="_blank" rel="noreferrer noopener">creating a social network</a> that integrates with ChatGPT.</li> <li>xkcd’s annual <a aria-label=" (opens in a new tab)" href="https://xkcd.com/3074/" target="_blank" rel="noreferrer noopener">belated April Fools’ joke</a> on push notifications is a masterpiece.&nbsp;</li> <li>Mozilla is looking past its Thunderbird email client to Thundermail Pro, a full email service that’s designed to <a aria-label=" (opens in a new tab)" href="https://www.techradar.com/pro/mozilla-launching-thundermail-email-service-to-take-on-gmail-microsoft-365" target="_blank" rel="noreferrer noopener">compete with Gmail</a>. It will include a calendaring service and an AI tool for help writing messages.</li> </ul> <h2 class="wp-block-heading">Quantum Computing</h2> <ul class="wp-block-list"> <li><a aria-label=" (opens in a new tab)" href="https://www.nature.com/articles/s41586-025-08801-w" target="_blank" rel="noreferrer noopener">Quantum messages have been sent</a> over <a aria-label=" (opens in a new tab)" href="https://phys.org/news/2025-04-quantum-messages-km-infrastructure.html" target="_blank" rel="noreferrer noopener">commercial communications infrastructure</a>. The distance (254 km) almost doesn’t matter; what’s more important is that the experiment used commercial optical fiber with no cooling or other quantum-specific support.</li> <li>An Australian company has <a aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2025-04-quantum-based-accurate-traditional-gps.html" target="_blank" rel="noreferrer noopener">developed</a> an alternative to GPS that uses quantum sensors to pinpoint locations based on the Earth’s <a aria-label=" (opens in a new tab)" href="https://arxiv.org/abs/2504.08167" target="_blank" rel="noreferrer noopener">magnetic field</a>. The device doesn’t emit signals, can filter out noise, and unlike current GPS systems, isn’t vulnerable to outages or attacks.&nbsp;</li> <li>Phasecraft has <a aria-label=" (opens in a new tab)" href="https://thenextweb.com/news/phasecraft-makes-quantum-simulations-10x-more-efficient" target="_blank" rel="noreferrer noopener">developed</a> an algorithm that makes quantum simulations more efficient. This advance could help quantum computers to model chemical reactions and create new materials.</li> </ul> <h2 class="wp-block-heading">Robotics</h2> <ul class="wp-block-list"> <li>Hugging Face has <a aria-label=" (opens in a new tab)" href="https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition" target="_blank" rel="noreferrer noopener">acquired</a> Pollen Robotics and is planning to sell robots. Its first offering, <a aria-label=" (opens in a new tab)" href="https://www.pollen-robotics.com/" target="_blank" rel="noreferrer noopener">Reachy 2</a>, is a humanoid robot that can be programmed using Hugging Face’s <a aria-label=" (opens in a new tab)" href="https://huggingface.co/lerobot" target="_blank" rel="noreferrer noopener">LeRobot</a> models.</li> <li><a aria-label=" (opens in a new tab)" href="https://arstechnica.com/science/2025/04/robobee-sticks-the-landing/" target="_blank" rel="noreferrer noopener">RoboBee</a> is a tiny flying robot (roughly an inch long) that can land safely on a leaf.</li> </ul> <hr class="wp-block-separator has-alpha-channel-opacity"/> <p><em>On May 8, O’Reilly Media will be hosting </em><strong>Coding with AI: The End of Software Development as We Know It</strong><em>—a live virtual tech conference spotlighting how AI is already supercharging developers, boosting productivity, and providing real value to their organizations. <a href="https://www.oreilly.com/CodingwithAI/" target="_blank" rel="noreferrer noopener">Register for free here</a>.</em></p> ]]>
</content:encoded>
</item>
<item>
<title>“Death by 1,000 Pilots”</title>
<link>https://www.oreilly.com/radar/death-by-1000-pilots/</link>
<pubDate>Tue, 29 Apr 2025 10:39:05 +0000</pubDate>
<dc:creator>
<![CDATA[ Tim O’Reilly and Mike Loukides ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Narrating Our Work—AI ]]>
</category>
<category>
<![CDATA[ Research ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.review/radar/?p=16572</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/binary-1695476_1920_crop-b676f0f03ac1cbb1eca82256d4b1b338-1.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ Getting from Experimentation to Production ]]>
</custom:subtitle>
<description>
<![CDATA[ Most companies find that the biggest challenge to AI is taking a promising experiment, demo, or proof-of-concept and bringing it to market. McKinsey Digital Analyst Rodney Zemmel sums this up: It’s “so easy to fire up a pilot that you can get stuck in this ‘death by 1,000 pilots’ approach.” It’s easy to see AI’s [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>Most companies find that the biggest challenge to AI is taking a promising experiment, demo, or proof-of-concept and bringing it to market. McKinsey Digital Analyst Rodney Zemmel sums this up: It’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/rewiring-for-the-era-of-gen-ai" target="_blank">“so easy to fire up a pilot that you can get stuck in this ‘death by 1,000 pilots’ approach.”</a> It’s easy to see AI’s potential, come up with some ideas, and spin up dozens (if not thousands) of pilot projects. However, the issue isn’t just the number of pilots; it’s also the difficulty of getting a pilot into production, something called “proof of concept purgatory” by Hugo Bowne-Anderson, and also discussed by Chip Huyen, Hamel Husain, and many other O’Reilly authors. Our work focuses on the challenges that come with bringing PoCs to production, such as scaling AI infrastructure, improving AI system reliability, and producing business value.</p> <p>Bringing products to production includes keeping them up to date with the newest technologies for building agentic AI systems, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.marktechpost.com/2024/09/22/rag-ai-agents-and-agentic-rag-an-in-depth-review-and-comparative-analysis-of-intelligent-ai-systems/" target="_blank">RAG</a>, GraphRAG, and MCP. We’re also following the development of reasoning models such as DeepSeek R1, Alibaba’s QwQ, Open AI’s 4o1 and 4o3, Google’s Gemini 2, and a growing number of other models. These models increase their accuracy by planning how to solve problems in advance.</p> <p>Developers also have to consider whether to use APIs from the major providers like Open AI, Anthropic, and Google&nbsp; or rely on open models, including Google’s Gemma, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.stateof.ai/" target="_blank">Meta’s Llama</a>, DeepSeek’s R1, and the many small language models that are derived (or “distilled”) from larger models.&nbsp; Many of these smaller models can run locally, without GPUs; some can run on limited hardware, like cell phones. The ability to run models locally gives AI developers options that didn’t exist a year or two ago. We are helping developers understand how to put those options to use.</p> <p>A final development is a change in the way software developers write code.&nbsp; Programmers increasingly rely on AI assistants to write code, and are also using AI for testing and debugging. Far from being the “end of programming,” this development means that software developers will become more efficient, able to develop more software for tasks that we haven’t yet automated and tasks we haven’t yet even imagined. The term “vibe coding” has captured the popular imagination, but using AI assistants appropriately requires discipline–and we’re only now understanding what that “discipline” means. As Steve Yegge says, you have to demand that the AI writes code that meets your quality standards as an engineer.</p> <p>AI assisted coding is only the tip of the iceberg, though. O’Reilly author Phillip Carter points out that LLMs and traditional software are good at different things. Understanding how to meld the two into an effective application requires a new approach to software architecture, debugging and ‘evals’, downstream monitoring and observability, and operations at scale. The internet’s dominant services have built using systems that provide rich feedback loops and accumulating data; these systems of control and optimization will necessarily be different as AI takes center stage.</p> <p>The challenge of achieving AI’s full potential is not just true for programming. AI is changing content creation, design, marketing, sales, corporate learning, and even internal management processes; the challenge will be building effective tools with AI, and both employees and customers will need to learn to use those new tools effectively. </p> <p>Helping our customers keep up with this avalanche of innovation, all the while turning exciting pilots into effective implementation: That’s our work in one sentence.</p> ]]>
</content:encoded>
</item>
<item>
<title>Context Serialization</title>
<link>https://www.oreilly.com/radar/context-serialization/</link>
<pubDate>Tue, 29 Apr 2025 09:56:59 +0000</pubDate>
<dc:creator>
<![CDATA[ Mike Loukides ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Signals ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.review/radar/?p=16570</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/banner-1033936_1920_crop-2d65f23a9b564608df7a34ef430a7fba-1.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ What does transferring context enable? ]]>
</custom:subtitle>
<description>
<![CDATA[ In a recent edition of The Sequence Engineering newsletter, “Why Did MCP Win?,” the authors point to context serialization and exchange as a reason—perhaps the most important reason—why everyone’s talking about the Model Context Protocol. I was puzzled by this—I’ve read a lot of technical and semitechnical posts about MCP and haven’t seen context serialization [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>In a recent edition of <em>The Sequence Engineering </em>newsletter, “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thesequence.substack.com/p/the-sequence-engineering-524-why" target="_blank">Why Did MCP Win?</a>,” the authors point to context serialization and exchange as a reason—perhaps the most important reason—why everyone’s talking about the Model Context Protocol. I was puzzled by this—I’ve read a lot of technical and semitechnical posts about MCP and haven’t seen context serialization mentioned. There are tutorials, lists of available MCP servers, and much more but nothing that mentions context serialization itself. I was even more puzzled after reading through the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://spec.modelcontextprotocol.io/specification/2025-03-26/" target="_blank">MCP specification</a>, in which the terms “context serialization” and “context exchange” don’t appear.</p> <p>What’s going on? The authors of the <em>Sequence Engineering</em> piece found the bigger picture, something more substantial than just using MCP to let Claude control Ableton. (Though that’s fun. Suno, beware!) It’s not just about letting language models drive traditional applications through a standard API. There isn’t a separate section on context serialization because all of MCP is about context serialization. That’s why it’s called the Model Context Protocol. Yes, it provides ways for applications to tell models about their capabilities so that agents can use those capabilities to complete a task. But it also gives models the means to share the current context with other applications that can make use of it. For traditional applications like GitHub, sharing context is meaningless. For the latest generation of applications that use networks of models, sharing context opens up new possibilities.</p> <p>Here’s a relatively simple example. You may be using AI to write a program. You add a new feature, test it, and it works. What happens next? From within your IDE, you can call traditional applications like Git to commit the changes—not a big deal, and some AI tools like Aider can already do that. But you also want to send a message to your manager and team members describing the project’s current state. Your AI-enhanced IDE might be able to generate an email. But Gmail has its own integrations with Gemini for writing email, and you’d prefer to use that. So your IDE can package everything relevant about your context and send it to Gemini, with instructions to decide what’s important, generate the message, and send the message via Gmail after it has been created. That’s different: Instead of an AI using a traditional application, now we have two AIs collaborating to complete a task. There can even be a conversation between the AIs about what to say in the message. (And you need to confirm that the result meets your expectations—vibe emailing to a boss seems like an antipattern.)</p> <p>Now we can start talking about networks of AIs working together. Here’s an example that’s only somewhat more complex. Imagine an AI application that helps farmers plan what they will plant. That application might want to use:</p> <ul class="wp-block-list"> <li>An economics service to forecast crop prices</li> <li>A service to forecast seed prices</li> <li>A service to forecast fertilizer prices</li> <li>A service to forecast fuel prices</li> <li>A weather service</li> <li>An agronomy model that predicts what crops will grow well at the farm’s location</li> </ul> <p>The application would probably require several more services that I can’t imagine–is there an entomology model that can forecast insect infestations? (Yes, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://onlinelibrary.wiley.com/doi/10.1111/jen.13227" target="_blank">there is</a>.) AI can already do a good job of predicting weather, and the financial industry is using AI to do economic modeling. One could imagine doing this all on a giant, “know everything” LLM (maybe GPT-6 or 7). But one thing we’re learning is that smaller, specialized models often outperform large generalist models in their areas of specialization. An AI that models crop prices should have access to a lot of important data that isn’t public. So should models that forecast seed prices, fertilizer prices, and fuel prices. All of these models are probably subscription-based services. It’s likely that a large farming business or cooperative would develop proprietary in-house models.</p> <p>The farmer’s AI needs to gather information from these specialized models by sending context to them: what the farmer wants to know, of course, but also the location of the fields, weather patterns over the past year, the farm’s production over the past few years, the farm’s technological capabilities, the availability of resources like water, and more. Furthermore, it’s not just a matter of asking each of these models a question, getting the answers, and generating a result; a conversation needs to happen between the specialist AIs because each answer will influence the others. It may be possible to predict the weather without knowing about economics, but you can’t do agricultural economics if you don’t understand the weather. This is where MCP’s value really lies. Building an application that asks models questions? That’s definitely useful, but any high school student can build an app that sends a prompt to ChatGPT and screen-scrapes the results. Anthropic’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://docs.anthropic.com/en/docs/agents-and-tools/computer-use" target="_blank">Computer Use</a> API goes a step further by automating the clicking and screen-scraping. The real value is in connecting models to each other so they can have conversations—so that a model that predicts the price of corn can discover weather forecasts for the coming year. We can build networks of AI models and agents. That’s what MCP supports. We couldn’t imagine this application just a few years ago. Now we can’t just imagine it, we can start building it. As Blaise Agüera y Arcas <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.noemamag.com/ai-is-evolving-and-changing-our-understanding-of-intelligence/" target="_blank">argues</a>, intelligence is collective and social. MCP gives us the tools to build artificial social intelligence.</p> <p>The industry has been talking about agents for some time now—dozens of years, really. The most recent burst of agentic discussion started just over a year ago. For the past year we’ve had models that were good enough, but we were missing an important piece of the puzzle: the ability to send context from one model to another. MCP provides some of the missing pieces. Google’s new <a href="https://github.com/google/A2A" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">A2A</a> protocol provides more of them. That’s what context serialization is all about, and that’s what it enables: networks of collaborating AIs, each acting as a specialist. Now, the only question is: What will we build?</p> ]]>
</content:encoded>
</item>
<item>
<title>Vibe Coding, Vibe Checking, and Vibe Blogging</title>
<link>https://www.oreilly.com/radar/vibe-coding-vibe-checking-and-vibe-blogging/</link>
<pubDate>Tue, 22 Apr 2025 10:02:10 +0000</pubDate>
<dc:creator>
<![CDATA[ Philip Guo ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Deep Dive ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.review/radar/?p=16561</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/digital-1742679_1920_crop-a5c269a22dfb950805a0b3c570fd20d6-1.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ Using Generative AI for Personal-Scale Projects ]]>
</custom:subtitle>
<description>
<![CDATA[ For the past decade and a half, I&#8217;ve been exploring the intersection of technology, education, and design as a professor of cognitive science and design at UC San Diego. Some of you might have read my recent piece for O&#8217;Reilly Radar where I detailed my journey adding AI chat capabilities to Python Tutor, the free [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>For the past decade and a half, I&#8217;ve been exploring the intersection of technology, education, and design as a <a aria-label=" (opens in a new tab)" href="https://pg.ucsd.edu/" target="_blank" rel="noreferrer noopener">professor of cognitive science and design</a> at UC San Diego. Some of you might have read my recent piece for O&#8217;Reilly Radar where I detailed my journey <a aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/using-generative-ai-to-build-generative-ai/" target="_blank" rel="noreferrer noopener">adding AI chat capabilities to Python Tutor</a>, the free visualization tool that&#8217;s helped millions of programming students understand how code executes. That experience got me thinking about my evolving relationship with generative AI as both a tool and a collaborator.</p> <p>I&#8217;ve been intrigued by this emerging practice called &#8220;vibe coding,&#8221; a term coined by Andrej Karpathy that&#8217;s been making waves in tech circles. <a aria-label=" (opens in a new tab)" href="https://simonwillison.net/2025/Mar/19/vibe-coding/" target="_blank" rel="noreferrer noopener">Simon Willison describes it perfectly</a>: “When I talk about vibe coding I mean building software with an LLM without reviewing the code it writes.&#8221; The concept is both liberating and slightly terrifying—you describe what you need, the AI generates the code, and you simply run it without scrutinizing each line, trusting the overall &#8220;vibe&#8221; of what&#8217;s been created.</p> <p>My relationship with this approach has evolved considerably. In my early days of using AI coding assistants, I was that person who meticulously reviewed every single line, often rewriting significant portions. But as these tools have improved, I&#8217;ve found myself gradually letting go of the steering wheel in certain contexts. Yet I couldn&#8217;t fully embrace the pure &#8220;vibe coding&#8221; philosophy; the professor in me needed some quality assurance. This led me to develop what I&#8217;ve come to call &#8220;vibe checks&#8221;—strategic verification points that provide confidence without reverting to line-by-line code reviews. It&#8217;s a middle path that&#8217;s worked surprisingly well for my personal projects, and today I want to share some insights from that journey.</p> <h2 class="wp-block-heading"><strong>Vibe Coding in Practice: Converting 250 HTML Files to Markdown</strong></h2> <p>I&#8217;ve found myself increasingly turning to vibe coding for those one-off scripts that solve specific problems in my workflow. These are typically tasks where explaining my intent is actually easier than writing the code myself, especially for data processing or file manipulation jobs where I can easily verify the results.</p> <p>Let me walk you through a recent example that perfectly illustrates this approach. For a class I teach, I had students submit responses to a survey using a proprietary web app that provided an HTML export option. This left me with 250 HTML files containing valuable student feedback, but it was buried in a mess of unnecessary markup and styling code. What I really wanted was clean Markdown versions that preserved just the text content, section headers, and—critically—any hyperlinks students had included in their responses.</p> <p>Rather than writing this conversion script myself, I turned to Claude with a straightforward request: &#8220;Write me a Python script that converts these HTML files to Markdown, preserving text, basic formatting, and hyperlinks.&#8221; Claude suggested using the BeautifulSoup library (a solid choice) and generated a complete script that would process all files in a directory, creating a corresponding Markdown file for each HTML source.</p> <p>(In retrospect, I realized I probably could have used <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://pandoc.org/" target="_blank">Pandoc</a> for this conversion task. But in the spirit of vibe coding, I just went with Claude&#8217;s suggestion without overthinking it. Part of the appeal of vibe coding is bypassing that research phase where you compare different approaches—you just describe what you want and roll with what you get.)</p> <p>True to the vibe coding philosophy, I didn&#8217;t review the generated code line by line. I simply saved it as a Python file, ran it on my directory of 250 HTML files, and waited to see what happened. This &#8220;run and see&#8221; approach is what makes vibe coding both liberating and slightly nerve-wracking—you&#8217;re trusting the AI&#8217;s interpretation of your needs without verifying the implementation details.</p> <h2 class="wp-block-heading"><strong>Trust and Risk in Vibe Coding: Running Unreviewed Code</strong></h2> <p>The moment I hit &#8220;run&#8221; on that vibe-coded script, I realized something that might make many developers cringe: I was executing completely unreviewed code on my actual computer with real data. In traditional software development, this would be considered reckless at best. But the dynamics of trust feel different with modern AI tools like Claude 3.7 Sonnet, which has built up a reputation for generating reasonably safe and functional code.</p> <p>My rationalization was partly based on the script&#8217;s limited scope. It was just reading HTML files and creating new Markdown files alongside them—not deleting, modifying existing files, or sending data over the network. Of course, that&#8217;s assuming the code did exactly what I asked and nothing more! I had no guarantees that it didn&#8217;t include some unexpected behavior since I hadn&#8217;t looked at a single line.</p> <p>This highlights a trust relationship that&#8217;s evolving between developers and AI coding tools. I&#8217;m much more willing to vibe code with Claude or ChatGPT than I would be with an unknown AI tool from some obscure website. These established tools have reputations to maintain, and their parent companies have strong incentives to prevent their systems from generating malicious code.</p> <p>That said, I&#8217;d love to see operating systems develop a &#8220;restricted execution mode&#8221; specifically designed for vibe coding scenarios. Imagine being able to specify: &#8220;Run this Python script, but only allow it to CREATE new files in this specific directory, prevent it from overwriting existing files, and block internet access.&#8221; This lightweight sandboxing would provide peace of mind without sacrificing convenience. (I mention only restricting writes rather than reads because Python scripts typically need to read various system files from across the filesystem, making read restrictions impractical.)</p> <p>Why not just use VMs, containers, or cloud services? Because for personal-scale projects, the convenience of working directly on my own machine is hard to beat. Setting up Docker or uploading 250 HTML files to some cloud service introduces friction that defeats the purpose of quick, convenient vibe coding. What I want is to maintain that convenience while adding just enough safety guardrails.</p> <h2 class="wp-block-heading"><strong>Vibe Checks: Simple Scripts to Verify AI-Generated Code</strong></h2> <p>OK now come the &#8220;vibe checks.&#8221; As I mentioned earlier, the nice thing about these personal data processing tasks is that I can often get a sense of whether the script did what I intended just by examining the output. For my HTML-to-Markdown conversion, I could open up several of the resulting Markdown files and see if they contained the survey responses I expected. This manual spot-checking works reasonably well for 250 files, but what about 2,500 or 25,000? At that scale, I&#8217;d need something more systematic.</p> <p>This is where vibe checks come into play. A vibe check is essentially a simpler script that verifies a basic property of the output from your vibe-coded script. The key here is that it should be much simpler than the original task, making it easier to verify its correctness.</p> <p>For my HTML-to-Markdown conversion project, I realized I could use a straightforward principle: Markdown files should be smaller than their HTML counterparts since we&#8217;re stripping away all the tags. But if a Markdown file is dramatically smaller—say, less than 40% of the original HTML size—that might indicate incomplete processing or content loss.</p> <p>So I went back to Claude and vibe coded a check script. This script simply:</p> <ol class="wp-block-list"> <li>Found all corresponding HTML/Markdown file pairs</li> <li>Calculated the size ratio for each pair</li> <li>Flagged any Markdown file smaller than 40% of its HTML source</li> </ol> <p>And lo and behold, the vibe check caught several files where the conversion was incomplete! The original script had failed to properly extract content from certain HTML structures. I took these problematic files, went back to Claude, and had it refine the original conversion script to handle these edge cases.</p> <p>After a few iterations of this feedback loop—convert, check, identify issues, refine—I eventually reached a point where there were no more suspiciously small Markdown files (well, there were still a few below 40%, but manual inspection confirmed these were correct conversions of HTML files with unusually high markup-to-content ratios).</p> <p>Now you might reasonably ask: &#8220;If you&#8217;re vibe coding the vibe check script too, how do you know <em>that</em> script is correct?&#8221; Would you need a vibe check for your vibe check? And then a vibe check for that check? Well, thankfully, this recursive nightmare has a practical solution. The vibe check script is typically an order of magnitude simpler than the original task—in my case, just comparing file sizes rather than parsing complex HTML. This simplicity made it feasible for me to manually review and verify the vibe check code, even while avoiding reviewing the more complex original script.</p> <p>Of course, my file size ratio check isn&#8217;t perfect. It can&#8217;t tell me if the content was converted with the proper formatting or if all hyperlinks were preserved correctly. But it gave me a reasonable confidence that no major content was missing, which was my primary concern.</p> <h2 class="wp-block-heading"><strong>Vibe Coding + Vibe Checking: A Pragmatic Middle Ground</strong></h2> <p>The take-home message here is simple but powerful: When you&#8217;re vibe coding, always build in vibe checks. Ask yourself: &#8220;What simpler script could verify the correctness of my main vibe-coded solution?&#8221; Even an imperfect verification mechanism dramatically increases your confidence in results from code you never actually reviewed.</p> <p>This approach strikes a nice balance between the speed and creative flow of pure vibe coding and the reliability of more rigorous software development methodologies. Think of vibe checks as lightweight tests—not the comprehensive test suites you&#8217;d write for production code, but enough verification to catch obvious failures without disrupting your momentum.</p> <p>What excites me about the future is the potential for AI coding tools to suggest appropriate vibe checks automatically. Imagine if Claude or similar tools could not only generate your requested script but also proactively offer: &#8220;Here&#8217;s a simple verification script you might want to run afterward to ensure everything worked as expected.&#8221; I suspect if I had specifically asked for this, Claude could have suggested the file size comparison check, but having this built into the system&#8217;s default behavior would be incredibly valuable. I can envision specialized AI coding assistants that operate in a semi-autonomous mode—writing code, generating appropriate checks, running those checks, and involving you only when human verification is truly needed.</p> <p>Combine this with the kind of sandboxed execution environment I mentioned earlier, and you&#8217;d have a vibe coding experience that&#8217;s both freeing and trustworthy—powerful enough for real work but with guardrails that prevent catastrophic mistakes.</p> <p>And now for the meta twist: This entire blog post was itself the product of &#8220;vibe blogging.&#8221; At the start of our collaboration, I uploaded my previous O&#8217;Reilly article,&#8221;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/using-generative-ai-to-build-generative-ai/" target="_blank">Using Generative AI to Build Generative AI</a>&#8221; as a reference document. This gave Claude the opportunity to analyze my writing style, tone, and typical structure—much like how a human collaborator might read my previous work before helping me write something new.</p> <p>Instead of writing the entire post in one go, I broke it down into sections and provided Claude with an outline for each section one at a time. For every section, I included key points I wanted to cover and sometimes specific phrasings or concepts to include. Claude then expanded these outlines into fully formed sections written in my voice. After each section was drafted, I reviewed it—my own version of a &#8220;vibe check&#8221;—providing feedback and requesting revisions until it matched what I wanted to say and how I wanted to say it.</p> <p>This iterative, section-by-section approach mirrors the vibe coding methodology I’ve discussed throughout this post. I didn&#8217;t need to write every sentence myself, but I maintained control over the direction, messaging, and final approval. The AI handled the execution details based on my high-level guidance, and I performed verification checks at strategic points rather than micromanaging every word.</p> <p>What&#8217;s particularly interesting is how this process demonstrates the same principles of trust, verification, and iteration that I advocated for in vibe coding. I trusted Claude to generate content in my style based on my outlines, but I verified each section before moving to the next. When something didn&#8217;t quite match my intent or tone, we iterated until it did. This balanced approach—leveraging AI capabilities while maintaining human oversight—seems to be the sweet spot for collaborative creation, whether you&#8217;re generating code or content.</p> <h2 class="wp-block-heading"><strong>Epilogue: Behind the Scenes with Claude</strong></h2> <p><em>[Claude speaking]</em></p> <p>Looking back at our vibe blogging experiment, I should acknowledge that Philip noted the final product doesn&#8217;t fully capture his authentic voice, despite having his O&#8217;Reilly article as a reference. But in keeping with the vibe philosophy itself, he chose not to invest excessive time in endless refinements—accepting good-enough rather than perfect.</p> <p>Working section-by-section without seeing the full structure upfront created challenges, similar to painting parts of a mural without seeing the complete design. I initially fell into the trap of copying his outline verbatim rather than transforming it properly.</p> <p>This collaboration highlights both the utility and limitations of AI-assisted content creation. I can approximate writing styles and expand outlines but still lack the lived experience that gives human writing its authentic voice. The best results came when Philip provided clear direction and feedback.</p> <p>The meta-example perfectly illustrates the core thesis: Generative AI works best when paired with human guidance, finding the right balance between automation and oversight. &#8220;Vibe blogging&#8221; has value for drafts and outlines, but like &#8220;vibe coding,&#8221; some form of human verification remains essential to ensure the final product truly represents what you want to say.</p> <p><em>[Philip speaking so that humans get the final word…for now]</em></p> <p>OK, this is the only part that I wrote by hand: My parting thought when reading over this post is that I&#8217;m not proud of the writing quality (sorry Claude!), but if it weren&#8217;t for an AI tool like Claude, I would not have written it in the first place due to lack of time and energy. I had enough energy today to outline some rough ideas, then let Claude do the “vibe blogging” for me, but not enough to fully write, edit, and fret over the wording of a full 2,500-word blog post all by myself. Thus, just like with vibe coding, one of the great joys of “vibe-*ing” is that it greatly lowers the activation energy of getting started on creative personal-scale prototypes and tinkering-style projects. To me, that&#8217;s pretty inspiring.</p> ]]>
</content:encoded>
</item>
<item>
<title>A Field Guide to Rapidly Improving AI Products</title>
<link>https://www.oreilly.com/radar/a-field-guide-to-rapidly-improving-ai-products/</link>
<pubDate>Tue, 15 Apr 2025 10:16:01 +0000</pubDate>
<dc:creator>
<![CDATA[ Hamel Husain ]]>
</dc:creator>
<category>
<![CDATA[ AI & ML ]]>
</category>
<category>
<![CDATA[ Artificial Intelligence ]]>
</category>
<category>
<![CDATA[ Research ]]>
</category>
<guid isPermaLink="false">https://www.oreilly.review/radar/?p=16531</guid>
<media:content url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/artificial-intelligence-3382507_1920_crop-dfe2b03f3e39775ad8cb072267bd6ae2.jpg" medium="image" type="image/jpeg"/>
<custom:subtitle>
<![CDATA[ Evaluation Methods, Data-Driven Improvement, and Experimentation Techniques from 30+ Production Implementations ]]>
</custom:subtitle>
<description>
<![CDATA[ Most AI teams focus on the wrong things. Here’s a common scene from my consulting work: AI TEAMHere’s our agent architecture—we’ve got RAG here, a router there, and we’re using this new framework for… ME[Holding up my hand to pause the enthusiastic tech lead]Can you show me how you’re measuring if any of this actually [&#8230;] ]]>
</description>
<content:encoded>
<![CDATA[ <p>Most AI teams focus on the wrong things. Here’s a common scene from my consulting work:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><strong>AI TEAM<br></strong>Here’s our agent architecture—we’ve got RAG here, a router there, and we’re using this new framework for…<br><br><strong>ME</strong><br>[Holding up my hand to pause the enthusiastic tech lead]<br>Can you show me how you’re measuring if any of this actually works?<br><br><em>… Room goes quiet</em><br></p> </blockquote> <p>This scene has played out dozens of times over the last two years. Teams invest weeks building complex AI systems but can’t tell me if their changes are helping or hurting.</p> <p>This isn’t surprising. With new tools and frameworks emerging weekly, it’s natural to focus on tangible things we can control—which vector database to use, which LLM provider to choose, which agent framework to adopt. But after helping 30+ companies build AI products, I’ve discovered that the teams who succeed barely talk about tools at all. Instead, they obsess over measurement and iteration.</p> <p>In this post, I’ll show you exactly how these successful teams operate. While every situation is unique, you’ll see patterns that apply regardless of your domain or team size. Let’s start by examining the most common mistake I see teams make—one that derails AI projects before they even begin.</p> <h2 class="wp-block-heading">The Most Common Mistake: Skipping Error Analysis</h2> <p>The “tools first” mindset is the most common mistake in AI development. Teams get caught up in architecture diagrams, frameworks, and dashboards while neglecting the process of actually understanding what’s working and what isn’t.</p> <p>One client proudly showed me this evaluation dashboard:</p> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/dashboard.png" alt="" /><figcaption class="wp-element-caption">The kind of dashboard that foreshadows failure</figcaption></figure> <p>This is the “tools trap”—the belief that adopting the right tools or frameworks (in this case, generic metrics) will solve your AI problems. Generic metrics are worse than useless—they actively impede progress in two ways:</p> <p>First, they create a false sense of measurement and progress. Teams think they’re data-driven because they have dashboards, but they’re tracking vanity metrics that don’t correlate with real user problems. I’ve seen teams celebrate improving their “helpfulness score” by 10% while their actual users were still struggling with basic tasks. It’s like optimizing your website’s load time while your checkout process is broken—you’re getting better at the wrong thing.</p> <p>Second, too many metrics fragment your attention. Instead of focusing on the few metrics that matter for your specific use case, you’re trying to optimize multiple dimensions simultaneously. When everything is important, nothing is.</p> <p>The alternative? Error analysis: the single most valuable activity in AI development and consistently the highest-ROI activity. Let me show you what effective error analysis looks like in practice.</p> <h3 class="wp-block-heading">The Error Analysis Process</h3> <p>When Jacob, the founder of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://nurtureboss.io/" target="_blank">Nurture Boss</a>, needed to improve the company&#8217;s apartment-industry AI assistant, his team built a simple viewer to examine conversations between their AI and users. Next to each conversation was a space for open-ended notes about failure modes.</p> <p>After annotating dozens of conversations, clear patterns emerged. Their AI was struggling with date handling—failing 66% of the time when users said things like “Let’s schedule a tour two weeks from now.”</p> <p>Instead of reaching for new tools, they: </p> <ol class="wp-block-list"> <li>Looked at actual conversation logs </li> <li>Categorized the types of date-handling failures </li> <li>Built specific tests to catch these issues </li> <li>Measured improvement on these metrics</li> </ol> <p>The result? Their date handling success rate improved from 33% to 95%.</p> <p>Here’s Jacob explaining this process himself:</p> <figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper"> <iframe loading="lazy" title="Error Analysis: The Highest ROI Technique In AI Engineering" width="500" height="281" src="https://www.youtube.com/embed/e2i6JbU2R-s?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> </div></figure> <h3 class="wp-block-heading">Bottom-Up Versus&nbsp;Top-Down Analysis</h3> <p>When identifying error types, you can take either a “top-down” or “bottom-up” approach.</p> <p>The top-down approach starts with common metrics like “hallucination” or “toxicity” plus metrics unique to your task. While convenient, it often misses domain-specific issues.</p> <p>The more effective bottom-up approach forces you to look at actual data and let metrics naturally emerge. At Nurture Boss, we started with a spreadsheet where each row represented a conversation. We wrote open-ended notes on any undesired behavior. Then we used an LLM to build a taxonomy of common failure modes. Finally, we mapped each row to specific failure mode labels and counted the frequency of each issue.</p> <p>The results were striking—just three issues accounted for over 60% of all problems:</p> <figure class="wp-block-image is-resized"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/pivot.png" alt="" style="width:579px;height:354px" /><figcaption class="wp-element-caption">Excel PivotTables are a simple tool, but they work!</figcaption></figure> <ul class="wp-block-list"> <li>Conversation flow issues (missing context, awkward responses)</li> <li>Handoff failures (not recognizing when to transfer to humans)</li> <li>Rescheduling problems (struggling with date handling)</li> </ul> <p>The impact was immediate. Jacob’s team had uncovered so many actionable insights that they needed several weeks just to implement fixes for the problems we’d already found.</p> <p>If you’d like to see error analysis in action, we recorded a <a href="https://youtu.be/qH1dZ8JLLdU" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">live walkthrough here</a>.</p> <p>This brings us to a crucial question: How do you make it easy for teams to look at their data? The answer leads us to what I consider the most important investment any AI team can make…</p> <h2 class="wp-block-heading">The Most Important AI Investment: A Simple Data Viewer</h2> <p>The single most impactful investment I’ve seen AI teams make isn’t a fancy evaluation dashboard—it’s building a customized interface that lets anyone examine what their AI is actually doing. I emphasize <em>customized</em> because every domain has unique needs that off-the-shelf tools rarely address. When reviewing apartment leasing conversations, you need to see the full chat history and scheduling context. For real estate queries, you need the property details and source documents right there. Even small UX decisions—like where to place metadata or which filters to expose—can make the difference between a tool people actually use and one they avoid.</p> <p>I’ve watched teams struggle with generic labeling interfaces, hunting through multiple systems just to understand a single interaction. The friction adds up: clicking through to different systems to see context, copying error descriptions into separate tracking sheets, switching between tools to verify information. This friction doesn’t just slow teams down—it actively discourages the kind of systematic analysis that catches subtle issues.</p> <p>Teams with thoughtfully designed data viewers iterate 10x faster than those without them. And here’s the thing: These tools can be built in hours using AI-assisted development (like Cursor or Loveable). The investment is minimal compared to the returns.</p> <p>Let me show you what I mean. Here’s the data viewer built for Nurture Boss (which I discussed earlier):</p> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/nboss_filter.png" alt="" /><figcaption class="wp-element-caption">Search and filter sessions.</figcaption></figure> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/nboss_annotate.png" alt="" /><figcaption class="wp-element-caption">Annotate and add notes.</figcaption></figure> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/nboss_analysis.png" alt="" /><figcaption class="wp-element-caption">Aggregate and count errors.</figcaption></figure> <p>Here’s what makes a good data annotation tool:</p> <ul class="wp-block-list"> <li>Show all context in one place. Don’t make users hunt through different systems to understand what happened.</li> <li>Make feedback trivial to capture. One-click correct/incorrect buttons beat lengthy forms.</li> <li>Capture open-ended feedback. This lets you capture nuanced issues that don’t fit into a predefined taxonomy.</li> <li>Enable quick filtering and sorting. Teams need to easily dive into specific error types. In the example above, Nurture Boss can quickly filter by the channel (voice, text, chat) or the specific property they want to look at quickly.</li> <li>Have hotkeys that allow users to navigate between data examples and annotate without clicking.</li> </ul> <p>It doesn’t matter what web frameworks you use—use whatever you’re familiar with. Because I’m a Python developer, my current favorite web framework is <a href="https://fastht.ml/docs/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">FastHTML</a> coupled with <a href="https://www.answer.ai/posts/2025-01-15-monsterui.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">MonsterUI</a> because it allows me to define the backend and frontend code in one small Python file.</p> <p>The key is starting somewhere, even if it’s simple. I’ve found custom web apps provide the best experience, but if you’re just beginning, a spreadsheet is better than nothing. As your needs grow, you can evolve your tools accordingly.</p> <p>This brings us to another counterintuitive lesson: The people best positioned to improve your AI system are often the ones who know the least about AI.</p> <h2 class="wp-block-heading">Empower Domain Experts To Write Prompts</h2> <p>I recently worked with an education startup building an interactive learning platform with LLMs. Their product manager, a learning design expert, would create detailed PowerPoint decks explaining pedagogical principles and example dialogues. She’d present these to the engineering team, who would then translate her expertise into prompts.</p> <p>But here’s the thing: Prompts are just English. Having a learning expert communicate teaching principles through PowerPoint only for engineers to translate that back into English prompts created unnecessary friction. The most successful teams flip this model by giving domain experts tools to write and iterate on prompts directly.</p> <h2 class="wp-block-heading">Build Bridges, Not Gatekeepers</h2> <p>Prompt playgrounds are a great starting point for this. Tools like Arize, LangSmith, and Braintrust let teams quickly test different prompts, feed in example datasets, and compare results. Here are some screenshots of these tools:</p> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/pp_phoenix2.png" alt="" /><figcaption class="wp-element-caption">Arize Phoenix</figcaption></figure> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/pp_langsmith.png" alt="" /><figcaption class="wp-element-caption">LangSmith</figcaption></figure> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/pp_bt.png" alt="" /><figcaption class="wp-element-caption">Braintrust</figcaption></figure> <p>But there’s a crucial next step that many teams miss: integrating prompt development into their application context. Most AI applications aren’t just prompts; they commonly involve RAG systems pulling from your knowledge base, agent orchestration coordinating multiple steps, and application-specific business logic. The most effective teams I’ve worked with go beyond stand-alone playgrounds. They build what I call <em>integrated prompt environments</em>—essentially admin versions of their actual user interface that expose prompt editing.</p> <p>Here’s an illustration of what an integrated prompt environment might look like for a real estate AI assistant:</p> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/ipe_before.png" alt="" /><figcaption class="wp-element-caption">The UI that users (real estate agents) see</figcaption></figure> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/ipe_after.png" alt="" /><figcaption class="wp-element-caption">The same UI, but with an “admin mode” used by the engineering and product team to iterate on the prompt and debug issues</figcaption></figure> <h3 class="wp-block-heading">Tips For Communicating With Domain Experts</h3> <p>There’s another barrier that often prevents domain experts from contributing effectively: unnecessary jargon. I was working with an education startup where engineers, product managers, and learning specialists were talking past each other in meetings. The engineers kept saying, “We’re going to build an agent that does XYZ,” when really the job to be done was writing a prompt. This created an artificial barrier—the learning specialists, who were the actual domain experts, felt like they couldn’t contribute because they didn’t understand “agents.”</p> <p>This happens everywhere. I’ve seen it with lawyers at legal tech companies, psychologists at mental health startups, and doctors at healthcare firms. The magic of LLMs is that they make AI accessible through natural language, but we often destroy that advantage by wrapping everything in technical terminology.</p> <p>Here’s a simple example of how to translate common AI jargon:</p> <figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-left" data-align="left"><strong>Instead of saying…</strong></td><td class="has-text-align-left" data-align="left"><strong>Say…</strong></td></tr><tr><td class="has-text-align-left" data-align="left">“We’re implementing a RAG approach.”</td><td class="has-text-align-left" data-align="left">“We’re making sure the model has the right context to answer questions.”</td></tr><tr><td class="has-text-align-left" data-align="left">“We need to prevent prompt injection.”</td><td class="has-text-align-left" data-align="left">“We need to make sure users can’t trick the AI into ignoring our rules.”</td></tr><tr><td class="has-text-align-left" data-align="left">“Our model suffers from hallucination issues.”</td><td class="has-text-align-left" data-align="left">“Sometimes the AI makes things up, so we need to check its answers.”</td></tr></tbody></table></figure> <p>This doesn’t mean dumbing things down—it means being precise about what you’re actually doing. When you say, “We’re building an agent,” what specific capability are you adding? Is it function calling? Tool use? Or just a better prompt? Being specific helps everyone understand what’s actually happening.</p> <p>There’s nuance here. Technical terminology exists for a reason: it provides precision when talking with other technical stakeholders. The key is adapting your language to your audience.</p> <p>The challenge many teams raise at this point is “This all sounds great, but what if we don’t have any data yet? How can we look at examples or iterate on prompts when we’re just starting out?” That’s what we’ll talk about next.</p> <h2 class="wp-block-heading">Bootstrapping Your AI With Synthetic Data Is Effective (Even With Zero Users)</h2> <p>One of the most common roadblocks I hear from teams is “We can’t do proper evaluation because we don’t have enough real user data yet.” This creates a chicken-and-egg problem—you need data to improve your AI, but you need a decent AI to get users who generate that data.</p> <p>Fortunately, there’s a solution that works surprisingly well: synthetic data. LLMs can generate realistic test cases that cover the range of scenarios your AI will encounter.</p> <p>As I wrote in my <a href="https://hamel.dev/blog/posts/llm-judge/#generating-data" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">LLM-as-a-Judge blog post</a>, synthetic data can be remarkably effective for evaluation. <a href="https://www.linkedin.com/in/bryan-bischof/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Bryan Bischof</a>, the former head of AI at Hex, put it perfectly:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>LLMs are surprisingly good at generating excellent &#8211; and diverse &#8211; examples of user prompts. This can be relevant for powering application features, and sneakily, for building Evals. If this sounds a bit like the Large Language Snake is eating its tail, I was just as surprised as you! All I can say is: it works, ship it.</em></p> </blockquote> <h3 class="wp-block-heading">A Framework for Generating Realistic Test Data</h3> <p>The key to effective synthetic data is choosing the right dimensions to test. While these dimensions will vary based on your specific needs, I find it helpful to think about three broad categories:</p> <ul class="wp-block-list"> <li>Features: What capabilities does your AI need to support?</li> <li>Scenarios: What situations will it encounter?</li> <li>User personas: Who will be using it and how?</li> </ul> <p>These aren’t the only dimensions you might care about—you might also want to test different tones of voice, levels of technical sophistication, or even different locales and languages. The important thing is identifying dimensions that matter for your specific use case.</p> <p>For a real estate CRM AI assistant I worked on with <a href="https://www.rechat.com/">Rechat</a>, we defined these dimensions like this:</p> <figure class="wp-block-image size-large is-resized"><img decoding="async" src="https://www.corp.oreilly.com/radar/wp-content/uploads/sites/3/2025/04/image.png" alt="" class="wp-image-16615" style="width:740px;height:433px" /></figure> <p>But having these dimensions defined is only half the battle. The real challenge is ensuring your synthetic data actually triggers the scenarios you want to test. This requires two things:</p> <ul class="wp-block-list"> <li>A test database with enough variety to support your scenarios</li> <li>A way to verify that generated queries actually trigger intended scenarios</li> </ul> <p>For Rechat, we maintained a test database of listings that we knew would trigger different edge cases. Some teams prefer to use an anonymized copy of production data, but either way, you need to ensure your test data has enough variety to exercise the scenarios you care about.</p> <p>Here’s an example of how we might use these dimensions with real data to generate test cases for the property search feature (this is just pseudo code, and very illustrative):</p> <pre class="wp-block-preformatted">def generate_search_query(scenario, persona, listing_db): """<em>Generate a realistic user query about listings</em>""" <em># Pull real listing data to ground the generation</em> sample_listings = listing_db.get_sample_listings( price_range=persona.price_range, location=persona.preferred_areas ) <em># Verify we have listings that will trigger our scenario</em> if scenario == "multiple_matches" and len(sample_listings) 0: raise ValueError("Found matches when testing no-match scenario") prompt = f""" You are an expert real estate agent who is searching for listings. You are given a customer type and a scenario. Your job is to generate a natural language query you would use to search these listings. Context: - Customer type: {persona.description} - Scenario: {scenario} Use these actual listings as reference: {format_listings(sample_listings)} The query should reflect the customer type and the scenario. Example query: Find homes in the 75019 zip code, 3 bedrooms, 2 bathrooms, price range $750k - $1M for an investor. """ return generate_with_llm(prompt)</pre> <p>This produced realistic queries like:</p> <figure class="wp-block-table"><table><thead><tr><th>Feature</th><th>Scenario</th><th>Persona</th><th>Generated Query</th></tr></thead><tbody><tr><td>property search</td><td>multiple matches</td><td>first_time_buyer</td><td>“Looking for 3-bedroom homes under $500k in the Riverside area. Would love something close to parks since we have young kids.”</td></tr><tr><td>market analysis</td><td>no matches</td><td>investor</td><td>“Need comps for 123 Oak St.&nbsp;Specifically interested in rental yield comparison with similar properties in a 2-mile radius.”</td></tr></tbody></table></figure> <p>The key to useful synthetic data is grounding it in real system constraints. For the real-estate AI assistant, this means:</p> <ul class="wp-block-list"> <li>Using real listing IDs and addresses from their database</li> <li>Incorporating actual agent schedules and availability windows</li> <li>Respecting business rules like showing restrictions and notice periods</li> <li>Including market-specific details like HOA requirements or local regulations</li> </ul> <p>We then feed these test cases through <a href="https://capacity.com/enterprise-search-software/?company=lucy.ai" target="_blank" rel="noreferrer noopener" aria-label="Lucy (now part of Capacity) (opens in a new tab)">Lucy (now part of Capacity)</a> and log the interactions. This gives us a rich dataset to analyze, showing exactly how the AI handles different situations with real system constraints. This approach helped us fix issues before they affected real users.</p> <p>Sometimes you don’t have access to a production database, especially for new products. In these cases, use LLMs to generate both test queries and the underlying test data. For a real estate AI assistant, this might mean creating synthetic property listings with realistic attributes—prices that match market ranges, valid addresses with real street names, and amenities appropriate for each property type. The key is grounding synthetic data in real-world constraints to make it useful for testing. The specifics of generating robust synthetic databases are beyond the scope of this post.</p> <h3 class="wp-block-heading">Guidelines for Using Synthetic Data</h3> <p>When generating synthetic data, follow these key principles to ensure it’s effective:</p> <ul class="wp-block-list"> <li>Diversify your dataset: Create examples that cover a wide range of features, scenarios, and personas. As I wrote in my <a aria-label=" (opens in a new tab)" href="https://hamel.dev/blog/posts/llm-judge/" target="_blank" rel="noreferrer noopener">LLM-as-a-Judge post</a>, this diversity helps you identify edge cases and failure modes you might not anticipate otherwise.</li> <li>Generate user inputs, not outputs: Use LLMs to generate realistic user queries or inputs, not the expected AI responses. This prevents your synthetic data from inheriting the biases or limitations of the generating model.</li> <li>Incorporate real system constraints: Ground your synthetic data in actual system limitations and data. For example, when testing a scheduling feature, use real availability windows and booking rules.</li> <li>Verify scenario coverage: Ensure your generated data actually triggers the scenarios you want to test. A query intended to test “no matches found” should actually return zero results when run against your system.</li> <li>Start simple, then add complexity: Begin with straightforward test cases before adding nuance. This helps isolate issues and establish a baseline before tackling edge cases.</li> </ul> <p>This approach isn’t just theoretical—it’s been proven in production across dozens of companies. What often starts as a stopgap measure becomes a permanent part of the evaluation infrastructure, even after real user data becomes available.</p> <p>Let’s look at how to maintain trust in your evaluation system as you scale.</p> <h2 class="wp-block-heading">Maintaining Trust In Evals Is Critical</h2> <p>This is a pattern I’ve seen repeatedly: Teams build evaluation systems, then gradually lose faith in them. Sometimes it’s because the metrics don’t align with what they observe in production. Other times, it’s because the evaluations become too complex to interpret. Either way, the result is the same: The team reverts to making decisions based on gut feeling and anecdotal feedback, undermining the entire purpose of having evaluations.</p> <p>Maintaining trust in your evaluation system is just as important as building it in the first place. Here’s how the most successful teams approach this challenge.</p> <h3 class="wp-block-heading">Understanding Criteria Drift</h3> <p>One of the most insidious problems in AI evaluation is “criteria drift”—a phenomenon where evaluation criteria evolve as you observe more model outputs. In their paper “<a href="https://arxiv.org/abs/2404.12272" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a>,” Shankar et al. describe this phenomenon:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>To grade outputs, people need to externalize and define their evaluation criteria; however, the process of grading outputs helps them to define that very criteria.</em></p> </blockquote> <p>This creates a paradox: You can’t fully define your evaluation criteria until you’ve seen a wide range of outputs, but you need criteria to evaluate those outputs in the first place. In other words, it is impossible to completely determine evaluation criteria prior to human judging of LLM outputs.</p> <p>I’ve observed this firsthand when working with Phillip Carter at Honeycomb on the company’s <a href="https://www.honeycomb.io/blog/introducing-query-assistant" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Query Assistant</a> feature. As we evaluated the AI’s ability to generate database queries, Phillip noticed something interesting:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>Seeing how the LLM breaks down its reasoning made me realize I wasn’t being consistent about how I judged certain edge cases.</em></p> </blockquote> <p>The process of reviewing AI outputs helped him articulate his own evaluation standards more clearly. This isn’t a sign of poor planning—it’s an inherent characteristic of working with AI systems that produce diverse and sometimes unexpected outputs.</p> <p>The teams that maintain trust in their evaluation systems embrace this reality rather than fighting it. They treat evaluation criteria as living documents that evolve alongside their understanding of the problem space. They also recognize that different stakeholders might have different (sometimes contradictory) criteria, and they work to reconcile these perspectives rather than imposing a single standard.</p> <h3 class="wp-block-heading">Creating Trustworthy Evaluation Systems</h3> <p>So how do you build evaluation systems that remain trustworthy despite criteria drift? Here are the approaches I’ve found most effective:</p> <h4 class="wp-block-heading"><strong>1. Favor Binary Decisions Over Arbitrary Scales</strong></h4> <p>As I wrote in my <a href="https://hamel.dev/blog/posts/llm-judge/#why-are-simple-passfail-metrics-important" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">LLM-as-a-Judge post</a>, binary decisions provide clarity that more complex scales often obscure. When faced with a 1–5 scale, evaluators frequently struggle with the difference between a 3 and a 4, introducing inconsistency and subjectivity. What exactly distinguishes “somewhat helpful” from “helpful”? These boundary cases consume disproportionate mental energy and create noise in your evaluation data. And even when businesses use a 1–5 scale, they inevitably ask where to draw the line for “good enough” or to trigger intervention, forcing a binary decision anyway.</p> <p>In contrast, a binary pass/fail forces evaluators to make a clear judgment: Did this output achieve its purpose or not? This clarity extends to measuring progress—a 10% increase in passing outputs is immediately meaningful, while a 0.5-point improvement on a 5-point scale requires interpretation.</p> <p>I’ve found that teams who resist binary evaluation often do so because they want to capture nuance. But nuance isn’t lost—it’s just moved to the qualitative critique that accompanies the judgment. The critique provides rich context about why something passed or failed and what specific aspects could be improved, while the binary decision creates actionable clarity about whether improvement is needed at all.</p> <h4 class="wp-block-heading">2. Enhance Binary Judgments With Detailed Critiques</h4> <p>While binary decisions provide clarity, they work best when paired with detailed critiques that capture the nuance of why something passed or failed. This combination gives you the best of both worlds: clear, actionable metrics and rich contextual understanding.</p> <p>For example, when evaluating a response that correctly answers a user’s question but contains unnecessary information, a good critique might read:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>The AI successfully provided the market analysis requested (PASS), but included excessive detail about neighborhood demographics that wasn’t relevant to the investment question. This makes the response longer than necessary and potentially distracting.</em></p> </blockquote> <p>These critiques serve multiple functions beyond just explanation. They force domain experts to externalize implicit knowledge—I’ve seen legal experts move from vague feelings that something “doesn’t sound right” to articulating specific issues with citation formats or reasoning patterns that can be systematically addressed.</p> <p>When included as few-shot examples in judge prompts, these critiques improve the LLM’s ability to reason about complex edge cases. I’ve found this approach often yields 15%–20% higher agreement rates between human and LLM evaluations compared to prompts without example critiques. The critiques also provide excellent raw material for generating high-quality synthetic data, creating a flywheel for improvement.</p> <h4 class="wp-block-heading">3. Measure Alignment Between Automated Evals and Human Judgment</h4> <p>If you’re using LLMs to evaluate outputs (which is often necessary at scale), it’s crucial to regularly check how well these automated evaluations align with human judgment.</p> <p>This is particularly important given our natural tendency to over-trust AI systems. As Shankar et al. note in “<a href="https://arxiv.org/abs/2404.12272" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Who Validates the Validators?</a>,” the lack of tools to validate evaluator quality is concerning.</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>Research shows people tend to over-rely and over-trust AI systems. For instance, in one high profile incident, researchers from MIT posted a pre-print on arXiv claiming that GPT-4 could ace the MIT EECS exam. Within hours, [the] work [was] debunked. . .citing problems arising from over-reliance on GPT-4 to grade itself.</em></p> </blockquote> <p>This overtrust problem extends beyond self-evaluation. Research has shown that LLMs can be biased by simple factors like the ordering of options in a set or even seemingly innocuous formatting changes in prompts. Without rigorous human validation, these biases can silently undermine your evaluation system.</p> <p>When working with Honeycomb, we tracked agreement rates between our LLM-as-a-judge and Phillip’s evaluations:</p> <figure class="wp-block-image"><img decoding="async" src="https://hamel.dev/blog/posts/field-guide/images/score.png" alt="" /><figcaption class="wp-element-caption">Agreement rates between LLM evaluator and human expert. More details <a href="https://hamel.dev/blog/posts/evals/#automated-evaluation-w-llms" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">here</a>.</figcaption></figure> <p>It took three iterations to achieve &gt;90% agreement, but this investment paid off in a system the team could trust. Without this validation step, automated evaluations often drift from human expectations over time, especially as the distribution of inputs changes. You can <a href="https://hamel.dev/blog/posts/evals/#automated-evaluation-w-llms" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">read more about this here</a>.</p> <p>Tools like <a href="https://eugeneyan.com/writing/aligneval/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Eugene Yan’s AlignEval</a> demonstrate this alignment process beautifully. AlignEval provides a simple interface where you upload data, label examples with a binary “good” or “bad,” and then evaluate LLM-based judges against those human judgments. What makes it effective is how it streamlines the workflow—you can quickly see where automated evaluations diverge from your preferences, refine your criteria based on these insights, and measure improvement over time. This approach reinforces that alignment isn’t a one-time setup but an ongoing conversation between human judgment and automated evaluation.</p> <h3 class="wp-block-heading">Scaling Without Losing Trust</h3> <p>As your AI system grows, you’ll inevitably face pressure to reduce the human effort involved in evaluation. This is where many teams go wrong—they automate too much, too quickly, and lose the human connection that keeps their evaluations grounded.</p> <p>The most successful teams take a more measured approach:</p> <ol class="wp-block-list"> <li>Start with high human involvement: In the early stages, have domain experts evaluate a significant percentage of outputs.</li> <li>Study alignment patterns: Rather than automating evaluation, focus on understanding where automated evaluations align with human judgment and where they diverge. This helps you identify which types of cases need more careful human attention.</li> <li>Use strategic sampling: Rather than evaluating every output, use statistical techniques to sample outputs that provide the most information, particularly focusing on areas where alignment is weakest.</li> <li>Maintain regular calibration: Even as you scale, continue to compare automated evaluations against human judgment regularly, using these comparisons to refine your understanding of when to trust automated evaluations.</li> </ol> <p>Scaling evaluation isn’t just about reducing human effort—it’s about directing that effort where it adds the most value. By focusing human attention on the most challenging or informative cases, you can maintain quality even as your system grows.</p> <p>Now that we’ve covered how to maintain trust in your evaluations, let’s talk about a fundamental shift in how you should approach AI development roadmaps.</p> <h2 class="wp-block-heading">Your AI Roadmap Should Count Experiments, Not Features</h2> <p>If you’ve worked in software development, you’re familiar with traditional roadmaps: a list of features with target delivery dates. Teams commit to shipping specific functionality by specific deadlines, and success is measured by how closely they hit those targets.</p> <p>This approach fails spectacularly with AI.</p> <p>I’ve watched teams commit to roadmap objectives like “Launch sentiment analysis by Q2” or “Deploy agent-based customer support by end of year,” only to discover that the technology simply isn’t ready to meet their quality bar. They either ship something subpar to hit the deadline or miss the deadline entirely. Either way, trust erodes.</p> <p>The fundamental problem is that traditional roadmaps assume we know what’s possible. With conventional software, that’s often true—given enough time and resources, you can build most features reliably. With AI, especially at the cutting edge, you’re constantly testing the boundaries of what’s feasible.</p> <h3 class="wp-block-heading">Experiments Versus&nbsp;Features</h3> <p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.linkedin.com/in/bryan-bischof/" target="_blank">Bryan Bischof</a>, former head of AI at Hex, introduced me to what he calls a “capability funnel” approach to AI roadmaps. This strategy reframes how we think about AI development progress. Instead of defining success as shipping a feature, the capability funnel breaks down AI performance into progressive levels of utility. At the top of the funnel is the most basic functionality: Can the system respond at all? At the bottom is fully solving the user’s job to be done. Between these points are various stages of increasing usefulness.</p> <p>For example, in a query assistant, the capability funnel might look like: </p> <ol class="wp-block-list"> <li>Can generate syntactically valid queries (basic functionality)</li> <li>Can generate queries that execute without errors </li> <li>Can generate queries that return relevant results</li> <li>Can generate queries that match user intent</li> <li>Can generate optimal queries that solve the user’s problem (complete solution)</li> </ol> <p>This approach acknowledges that AI progress isn’t binary—it’s about gradually improving capabilities across multiple dimensions. It also provides a framework for measuring progress even when you haven’t reached the final goal.</p> <p>The most successful teams I’ve worked with structure their roadmaps around experiments rather than features. Instead of committing to specific outcomes, they commit to a cadence of experimentation, learning, and iteration.</p> <p><a href="https://eugeneyan.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Eugene Yan</a>, an applied scientist at Amazon, shared how he approaches ML project planning with leadership—a process that, while originally developed for traditional machine learning, applies equally well to modern LLM development:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>Here’s a common timeline. First, I take two weeks to do a data feasibility analysis, i.e., “Do I have the right data?”&#8230;Then I take an additional month to do a technical feasibility analysis, i.e., “Can AI solve this?” After that, if it still works I’ll spend six weeks building a prototype we can A/B test.</em></p> </blockquote> <p>While LLMs might not require the same kind of feature engineering or model training as traditional ML, the underlying principle remains the same: time-box your exploration, establish clear decision points, and focus on proving feasibility before committing to full implementation. This approach gives leadership confidence that resources won’t be wasted on open-ended exploration, while giving the team the freedom to learn and adapt as they go.</p> <h3 class="wp-block-heading">The Foundation: Evaluation Infrastructure</h3> <p>The key to making an experiment-based roadmap work is having robust evaluation infrastructure. Without it, you’re just guessing whether your experiments are working. With it, you can rapidly iterate, test hypotheses, and build on successes.</p> <p>I saw this firsthand during the early development of GitHub Copilot. What most people don’t realize is that the team invested heavily in building sophisticated offline evaluation infrastructure. They created systems that could test code completions against a very large corpus of repositories on GitHub, leveraging unit tests that already existed in high-quality codebases as an automated way to verify completion correctness. This was a massive engineering undertaking—they had to build systems that could clone repositories at scale, set up their environments, run their test suites, and analyze the results, all while handling the incredible diversity of programming languages, frameworks, and testing approaches.</p> <p>This wasn’t wasted time—it was the foundation that accelerated everything. With solid evaluation in place, the team ran thousands of experiments, quickly identified what worked, and could say with confidence “This change improved quality by X%” instead of relying on gut feelings. While the upfront investment in evaluation feels slow, it prevents endless debates about whether changes help or hurt and dramatically speeds up innovation later.</p> <h3 class="wp-block-heading">Communicating This to Stakeholders</h3> <p>The challenge, of course, is that executives often want certainty. They want to know when features will ship and what they’ll do. How do you bridge this gap?</p> <p>The key is to shift the conversation from outputs to outcomes. Instead of promising specific features by specific dates, commit to a process that will maximize the chances of achieving the desired business outcomes.</p> <p>Eugene shared how he handles these conversations:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>I try to reassure leadership with timeboxes. At the end of three months, if it works out, then we move it to production. At any step of the way, if it doesn’t work out, we pivot.</em></p> </blockquote> <p>This approach gives stakeholders clear decision points while acknowledging the inherent uncertainty in AI development. It also helps manage expectations about timelines—instead of promising a feature in six months, you’re promising a clear understanding of whether that feature is feasible in three months.</p> <p>Bryan’s capability funnel approach provides another powerful communication tool. It allows teams to show concrete progress through the funnel stages, even when the final solution isn’t ready. It also helps executives understand where problems are occurring and make informed decisions about where to invest resources.</p> <h3 class="wp-block-heading">Build a Culture of Experimentation Through Failure Sharing</h3> <p>Perhaps the most counterintuitive aspect of this approach is the emphasis on learning from failures. In traditional software development, failures are often hidden or downplayed. In AI development, they’re the primary source of learning.</p> <p>Eugene operationalizes this at his organization through what he calls a “fifteen-five”—a weekly update that takes fifteen minutes to write and five minutes to read:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>In my fifteen-fives, I document my failures and my successes. Within our team, we also have weekly “no-prep sharing sessions” where we discuss what we’ve been working on and what we’ve learned. When I do this, I go out of my way to share failures.</em></p> </blockquote> <p>This practice normalizes failure as part of the learning process. It shows that even experienced practitioners encounter dead-ends, and it accelerates team learning by sharing those experiences openly. And by celebrating the process of experimentation rather than just the outcomes, teams create an environment where people feel safe taking risks and learning from failures.</p> <h3 class="wp-block-heading">A Better Way Forward</h3> <p>So what does an experiment-based roadmap look like in practice? Here’s a simplified example from a content moderation project Eugene worked on:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>I was asked to do content moderation. I said, “It’s uncertain whether we’ll meet that goal. It’s uncertain even if that goal is feasible with our data, or what machine learning techniques would work. But here’s my experimentation roadmap. Here are the techniques I’m gonna try, and I’m gonna update you at a two-week cadence.”</em></p> </blockquote> <p>The roadmap didn’t promise specific features or capabilities. Instead, it committed to a systematic exploration of possible approaches, with regular check-ins to assess progress and pivot if necessary.</p> <p>The results were telling:</p> <blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"> <p><em>For the first two to three months, nothing worked. . . .And then [a breakthrough] came out. . . .Within a month, that problem was solved. So you can see that in the first quarter or even four months, it was going nowhere. . . .But then you can also see that all of a sudden, some new technology&#8230;, some new paradigm, some new reframing comes along that just [solves] 80% of [the problem].</em></p> </blockquote> <p>This pattern—long periods of apparent failure followed by breakthroughs—is common in AI development. Traditional feature-based roadmaps would have killed the project after months of “failure,” missing the eventual breakthrough.</p> <p>By focusing on experiments rather than features, teams create space for these breakthroughs to emerge. They also build the infrastructure and processes that make breakthroughs more likely: data pipelines, evaluation frameworks, and rapid iteration cycles.</p> <p>The most successful teams I’ve worked with start by building evaluation infrastructure before committing to specific features. They create tools that make iteration faster and focus on processes that support rapid experimentation. This approach might seem slower at first, but it dramatically accelerates development in the long run by enabling teams to learn and adapt quickly.</p> <p>The key metric for AI roadmaps isn’t features shipped—it’s experiments run. The teams that win are those that can run more experiments, learn faster, and iterate more quickly than their competitors. And the foundation for this rapid experimentation is always the same: robust, trusted evaluation infrastructure that gives everyone confidence in the results.</p> <p>By reframing your roadmap around experiments rather than features, you create the conditions for similar breakthroughs in your own organization.</p> <h2 class="wp-block-heading">Conclusion</h2> <p>Throughout this post, I’ve shared patterns I’ve observed across dozens of AI implementations. The most successful teams aren’t the ones with the most sophisticated tools or the most advanced models—they’re the ones that master the fundamentals of measurement, iteration, and learning.</p> <p>The core principles are surprisingly simple:</p> <ul class="wp-block-list"> <li>Look at your data. Nothing replaces the insight gained from examining real examples. Error analysis consistently reveals the highest-ROI improvements.</li> <li>Build simple tools that remove friction. Custom data viewers that make it easy to examine AI outputs yield more insights than complex dashboards with generic metrics.</li> <li>Empower domain experts. The people who understand your domain best are often the ones who can most effectively improve your AI, regardless of their technical background.</li> <li>Use synthetic data strategically. You don’t need real users to start testing and improving your AI. Thoughtfully generated synthetic data can bootstrap your evaluation process.</li> <li>Maintain trust in your evaluations. Binary judgments with detailed critiques create clarity while preserving nuance. Regular alignment checks ensure automated evaluations remain trustworthy.</li> <li>Structure roadmaps around experiments, not features. Commit to a cadence of experimentation and learning rather than specific outcomes by specific dates.</li> </ul> <p>These principles apply regardless of your domain, team size, or technical stack. They’ve worked for companies ranging from early-stage startups to tech giants, across use cases from customer support to code generation.</p> <h3 class="wp-block-heading">Resources for Going Deeper</h3> <p>If you’d like to explore these topics further, here are some resources that might help:</p> <ul class="wp-block-list"> <li><a aria-label=" (opens in a new tab)" href="https://ai.hamel.dev/" target="_blank" rel="noreferrer noopener">My blog</a> for more content on AI evaluation and improvement. My other posts dive into more technical detail on topics such as constructing effective LLM judges, implementing evaluation systems, and other aspects of AI development.<sup>1</sup> Also check out the blogs of <a aria-label=" (opens in a new tab)" href="https://www.sh-reya.com/" target="_blank" rel="noreferrer noopener">Shreya Shankar</a> and <a aria-label=" (opens in a new tab)" href="https://eugeneyan.com/" target="_blank" rel="noreferrer noopener">Eugene Yan</a>, who are also great sources of information on these topics.</li> <li>A course I’m teaching, <a aria-label=" (opens in a new tab)" href="https://bit.ly/evals-ai" target="_blank" rel="noreferrer noopener">Rapidly Improve AI Products with Evals</a>, with Shreya Shankar. It provides hands-on experience with techniques such as error analysis, synthetic data generation, and building trustworthy evaluation systems, and includes practical exercises and personalized instruction through office hours.</li> <li>If you’re looking for hands-on guidance specific to your organization’s needs, you can learn more about working with me at <a aria-label=" (opens in a new tab)" href="https://parlance-labs.com/" target="_blank" rel="noreferrer noopener">Parlance Labs</a>.</li> </ul> <hr class="wp-block-separator has-css-opacity" /> <h3 class="wp-block-heading">Footnotes</h3> <ol class="wp-block-list"> <li>I write more broadly about machine learning, AI, and software development. Some posts that expand on these topics include “<a aria-label=" (opens in a new tab)" href="https://hamel.dev/blog/posts/evals/" target="_blank" rel="noreferrer noopener">Your AI Product Needs Evals</a>,” “<a aria-label=" (opens in a new tab)" href="https://hamel.dev/blog/posts/llm-judge/" target="_blank" rel="noreferrer noopener">Creating a LLM-as-a-Judge That Drives Business Results</a>,” and “<a aria-label=" (opens in a new tab)" href="https://applied-llms.org/" target="_blank" rel="noreferrer noopener">What We’ve Learned from a Year of Building with LLMs</a>.” You can see all my posts at <a aria-label=" (opens in a new tab)" href="https://hamel.dev/" target="_blank" rel="noreferrer noopener">hamel.dev</a>.</li> </ol> ]]>
</content:encoded>
</item>
</channel>
</rss>
<!-- 
Performance optimized by W3 Total Cache. Learn more: https://www.boldgrid.com/w3-total-cache/

Object Caching 195/217 objects using Memcached
Page Caching using Disk: Enhanced (Page is feed) 
Minified using Memcached
Database Caching using Memcached

Served from: www.oreilly.com @ 2025-05-29 15:38:28 by W3 Total Cache
 -->